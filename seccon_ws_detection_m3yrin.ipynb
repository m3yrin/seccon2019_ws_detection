{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seccon_ws_detection_m3yrin.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m3yrin/seccon2019_ws_detection/blob/master/seccon_ws_detection_m3yrin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1_5Busp3cYf",
        "colab_type": "text"
      },
      "source": [
        "# CNN model for 攻撃自動検知ワークショップ in SECCON 2019 \n",
        "\n",
        "https://connpass.com/event/159753/  \n",
        "seccon_ws_detection  \n",
        "\n",
        "auther : ＠m3yrin\n",
        "\n",
        "## Model Specification\n",
        "* Char-Base CNN Embedding\n",
        "* RNN (GRU) sequence modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za5eHvTQ4GbW",
        "colab_type": "text"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvdF94MkdMOq",
        "colab_type": "code",
        "outputId": "2026ecdb-1c74-4ec3-e162-6ea8be444ff9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone https://github.com/palloc/seccon_ws_detection.git\n",
        "!pip install allennlp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'seccon_ws_detection'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects:   5% (1/19)\u001b[K\rremote: Counting objects:  10% (2/19)\u001b[K\rremote: Counting objects:  15% (3/19)\u001b[K\rremote: Counting objects:  21% (4/19)\u001b[K\rremote: Counting objects:  26% (5/19)\u001b[K\rremote: Counting objects:  31% (6/19)\u001b[K\rremote: Counting objects:  36% (7/19)\u001b[K\rremote: Counting objects:  42% (8/19)\u001b[K\rremote: Counting objects:  47% (9/19)\u001b[K\rremote: Counting objects:  52% (10/19)\u001b[K\rremote: Counting objects:  57% (11/19)\u001b[K\rremote: Counting objects:  63% (12/19)\u001b[K\rremote: Counting objects:  68% (13/19)\u001b[K\rremote: Counting objects:  73% (14/19)\u001b[K\rremote: Counting objects:  78% (15/19)\u001b[K\rremote: Counting objects:  84% (16/19)\u001b[K\rremote: Counting objects:  89% (17/19)\u001b[K\rremote: Counting objects:  94% (18/19)\u001b[K\rremote: Counting objects: 100% (19/19)\u001b[K\rremote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects:   5% (1/17)\u001b[K\rremote: Compressing objects:  11% (2/17)\u001b[K\rremote: Compressing objects:  17% (3/17)\u001b[K\rremote: Compressing objects:  23% (4/17)\u001b[K\rremote: Compressing objects:  29% (5/17)\u001b[K\rremote: Compressing objects:  35% (6/17)\u001b[K\rremote: Compressing objects:  41% (7/17)\u001b[K\rremote: Compressing objects:  47% (8/17)\u001b[K\rremote: Compressing objects:  52% (9/17)\u001b[K\rremote: Compressing objects:  58% (10/17)\u001b[K\rremote: Compressing objects:  64% (11/17)\u001b[K\rremote: Compressing objects:  70% (12/17)\u001b[K\rremote: Compressing objects:  76% (13/17)\u001b[K\rremote: Compressing objects:  82% (14/17)\u001b[K\rremote: Compressing objects:  88% (15/17)\u001b[K\rremote: Compressing objects:  94% (16/17)\u001b[K\rremote: Compressing objects: 100% (17/17)\u001b[K\rremote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 19 (delta 1), reused 17 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (19/19), done.\n",
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.3)\n",
            "Collecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n",
            "Collecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 39.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.17.4)\n",
            "Collecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/86/7a/532fc167366797f66c732549490dcf13243077f15446115f3c0ad17e56b8/overrides-2.6.tar.gz\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.1.2)\n",
            "Collecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Requirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Collecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.1)\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/5c/e918d9f190baab8d55bad52840d8091dd5114cc99f03eaa6d72d404503cc/tensorboardX-1.9-py2.py3-none-any.whl (190kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 42.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.21.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.10.40)\n",
            "Collecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 37.6MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ca/2d9a5030eaf1bcd925dab392762b9709a7ad4bd486a90599d93cd79cb188/ftfy-5.6.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.9MB/s \n",
            "\u001b[?25hCollecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/3e/0c/940781dd49710f4b1f0650c450c9fd8491db0e1bffd99ebc36355607f96d/responses-0.10.9-py2.py3-none-any.whl\n",
            "Collecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.8MB/s \n",
            "\u001b[?25hCollecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/a6/e69e38f1f259fcf8532d8bd2c4bc88764f42d7b35a41423a7f4b035cc5ce/jsonnet-0.14.0.tar.gz (253kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 42.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n",
            "Collecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 42.7MB/s \n",
            "\u001b[?25hCollecting numpydoc>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/f3/7cfe4c616e4b9fe05540256cc9c6661c052c8a4cec2915732793b36e1843/numpydoc-0.9.1.tar.gz\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.0.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.12.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (42.0.2)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 36.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp) (2019.12.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.6.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.5)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.4.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.9.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.3)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (7.0.8)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (2.10.3)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (0.16.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.14.1)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.13.40)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask>=1.0.2->allennlp) (1.1.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->allennlp) (0.15.2)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.2)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (19.2)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.7.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Building wheels for collected packages: overrides, word2number, ftfy, parsimonious, jsonnet, numpydoc\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.6-cp36-none-any.whl size=5523 sha256=5848b5fb2a1396f712828e88493bbbb4074a3eda3852c80f7ab8b390c3fbe44c\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/86/49/c413319bcff638bdc13462c063c84d68e294d84514170c3744\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5588 sha256=6b03429775a421298be5a94c893531bf9434faefd376243a4ccc1f92c831aa43\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.6-cp36-none-any.whl size=44553 sha256=f016d033dbee7fb0e8c3ed78937372d780fd5ab9ed09a717130a57aaf63d2127\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/34/ce/cbb38d71543c408de56f3c5e26ce8ba495a0fa5a28eaaf1046\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42709 sha256=026f8de96859e0580566a6600eed1f6d69b43ee321a627f688f22d14c7ca3621\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.14.0-cp36-cp36m-linux_x86_64.whl size=3320339 sha256=47ceab6b85d3375020fcd83fdcb12efb29b7036b4943634139dbfb9baee7900d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/b7/83/985f0f758fbb34f14989a0fab86d18890d1cc5ae12f26967bc\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numpydoc: filename=numpydoc-0.9.1-cp36-none-any.whl size=31872 sha256=ce325b4f6576ff8b7aacad13ae0e7980b0510d4f703f9d50ae592bf1c5647882\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/30/d1/92a39ba40f21cb70e53f8af96eb98f002a781843c065406500\n",
            "Successfully built overrides word2number ftfy parsimonious jsonnet numpydoc\n",
            "Installing collected packages: flaky, sentencepiece, pytorch-transformers, overrides, conllu, word2number, tensorboardX, pytorch-pretrained-bert, ftfy, responses, parsimonious, jsonnet, jsonpickle, flask-cors, unidecode, numpydoc, allennlp\n",
            "Successfully installed allennlp-0.9.0 conllu-1.3.1 flaky-3.6.1 flask-cors-3.0.8 ftfy-5.6 jsonnet-0.14.0 jsonpickle-1.2 numpydoc-0.9.1 overrides-2.6 parsimonious-0.8.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.10.9 sentencepiece-0.1.85 tensorboardX-1.9 unidecode-1.1.1 word2number-1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecNA1XeQdQmz",
        "colab_type": "code",
        "outputId": "321092ca-3c8e-4df9-f7dc-2f08fb17947c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd seccon_ws_detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/seccon_ws_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK0ARpOC5akT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "9bb0a409-2848-4648-bb05-599a923ba2d9"
      },
      "source": [
        "!wget -P dataset https://raw.githubusercontent.com/palloc/seccon_ws_detection/score/dataset/level2_train.tsv"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-21 05:32:45--  https://raw.githubusercontent.com/palloc/seccon_ws_detection/score/dataset/level2_train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12683 (12K) [text/plain]\n",
            "Saving to: ‘dataset/level2_train.tsv’\n",
            "\n",
            "\rlevel2_train.tsv      0%[                    ]       0  --.-KB/s               \rlevel2_train.tsv    100%[===================>]  12.39K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-12-21 05:32:46 (167 MB/s) - ‘dataset/level2_train.tsv’ saved [12683/12683]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3K4SJtw2rai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filepath configs\n",
        "import os\n",
        "\n",
        "full_path = './'\n",
        "DATA_DIR = os.path.join(full_path, 'dataset')\n",
        "\n",
        "#TRAIN_FILE = 'level1_train.tsv'\n",
        "TRAIN_FILE = 'level2_train.tsv'\n",
        "TRAIN_PATH = os.path.join(DATA_DIR, TRAIN_FILE)\n",
        "\n",
        "AUG_FILE = 'train_aug.tsv'\n",
        "AUG_PATH = os.path.join(DATA_DIR, AUG_FILE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CHAcvy4LlCw",
        "colab_type": "text"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKReeNBevkdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import codecs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(seed=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3bwZ2YjLmrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load dataset.\n",
        "def load_dataset(path, num_augment = None):\n",
        "    dataset = []\n",
        "    X = []\n",
        "    y = []\n",
        "    with codecs.open(path, mode='r', encoding='utf-8') as fin:\n",
        "        dataset.extend(fin.readlines())\n",
        "    \n",
        "    for payload in dataset:\n",
        "        \n",
        "        \n",
        "        X.append(payload.split('\\t')[0])\n",
        "\n",
        "        label = payload.split('\\t')[1].replace('\\n', '')\n",
        "        label = int(label)\n",
        "        y.append(label)\n",
        "\n",
        "\n",
        "    # Data augmentation\n",
        "    # sampling random 2 indices and concatinating them.\n",
        "    if num_augment is not None:\n",
        "        X_aug = []\n",
        "        y_aug = []\n",
        "\n",
        "        ids_list = np.random.randint(len(X), size=(num_augment, 2))\n",
        "\n",
        "        for ids in ids_list:\n",
        "            X_concat = X[ids[0]] + X[ids[1]]\n",
        "\n",
        "            y_concat = 1 if int(y[ids[0]]) + int(y[ids[1]]) > 0 else 0\n",
        "\n",
        "            X_aug.append(X_concat)\n",
        "            y_aug.append(y_concat)\n",
        "        \n",
        "        # Also added original sequences\n",
        "        return  X_aug + X, y_aug + y\n",
        "\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxZXNriqLuBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = load_dataset(TRAIN_PATH)\n",
        "data_df = pd.DataFrame(zip(X, y))\n",
        "data_df.columns = ['X','y']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm7ieqj8wy8y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1d2a0ab5-facb-43ca-b221-be2ee2f50a79"
      },
      "source": [
        "data_df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: lipman@oasys.dt.navy.mil (Robert Lipman)</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Subject: CALL FOR PRESENTATIONS: Navy SciViz/V...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SPONSOR: NESS (Navy Engineering Software Syste...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This computer is occurred a error on the memory.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The purpose of the seminar is to present and e...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   X  y\n",
              "0     From: lipman@oasys.dt.navy.mil (Robert Lipman)  0\n",
              "1  Subject: CALL FOR PRESENTATIONS: Navy SciViz/V...  0\n",
              "2  SPONSOR: NESS (Navy Engineering Software Syste...  0\n",
              "3   This computer is occurred a error on the memory.  0\n",
              "4  The purpose of the seminar is to present and e...  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck9UYTm-wzvJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "c16cd1a8-7470-4cb5-8089-96cd4c15cbe2"
      },
      "source": [
        "data_df.describe()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>200.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.501255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                y\n",
              "count  200.000000\n",
              "mean     0.500000\n",
              "std      0.501255\n",
              "min      0.000000\n",
              "25%      0.000000\n",
              "50%      0.500000\n",
              "75%      1.000000\n",
              "max      1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqTwbP9ZxjhA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "819b2107-d864-4a48-fc19-6a990fccc9f5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# check the distribution of y\n",
        "data_df.y.value_counts().plot(kind=\"bar\")\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD1CAYAAACrz7WZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nd0lqyLhLUkPGXZIa+h9/5BcFpHm8eAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifvlluJmyR-8",
        "colab_type": "text"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvAQdvmPwf-q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5b1a98b2-417f-410f-bf3c-ae2907504823"
      },
      "source": [
        "num_augment = 400\n",
        "X, y = load_dataset(TRAIN_PATH, num_augment = num_augment)\n",
        "\n",
        "data_auged_df = pd.DataFrame(zip(X, y))\n",
        "data_auged_df.columns = ['X','y']\n",
        "data_auged_df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&gt;&gt;technical support over the phone, free softw...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>to select something necessary to make a necess...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;svg&gt;&lt;style&gt;&lt;img/src=x onerror=alert(1)// &lt;/b&gt;...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;/iframe&gt; /&gt;&lt;/textarea&gt;&lt;video&gt;&lt;source onerror=...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[ I'm thinking of making this post bi-weekly. ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   X  y\n",
              "0  >>technical support over the phone, free softw...  1\n",
              "1  to select something necessary to make a necess...  1\n",
              "2  <svg><style><img/src=x onerror=alert(1)// </b>...  1\n",
              "3  </iframe> /></textarea><video><source onerror=...  1\n",
              "4  [ I'm thinking of making this post bi-weekly. ...  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie7BlA_TwmS0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "f0d98c01-403a-4b00-c780-1f3a6aedbe25"
      },
      "source": [
        "data_auged_df.describe()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>600.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.691667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.462190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                y\n",
              "count  600.000000\n",
              "mean     0.691667\n",
              "std      0.462190\n",
              "min      0.000000\n",
              "25%      0.000000\n",
              "50%      1.000000\n",
              "75%      1.000000\n",
              "max      1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAv90JnDMBYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "f093e154-4653-4379-9269-7bdcc38c125b"
      },
      "source": [
        "data_auged_df.y.value_counts().plot(kind=\"bar\")\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD1CAYAAACrz7WZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEWcaUgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5mWEnxSMXNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_auged_df.to_csv(AUG_PATH, sep='\\t' , index = False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDKtqZjxMXKI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcce5a15-731a-49ad-d207-cc9fc8903d0d"
      },
      "source": [
        "!ls dataset"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "level1_test.tsv  level1_train.tsv  level2_train.tsv  train_aug.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoJTHRSIL_3h",
        "colab_type": "text"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7MZ71Vn2LdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import logging\n",
        "\n",
        "import itertools\n",
        "import json\n",
        "import logging\n",
        "import string\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Union, Tuple, Any\n",
        "\n",
        "from overrides import overrides\n",
        "from word2number.w2n import word_to_num\n",
        "\n",
        "import allennlp\n",
        "from allennlp.common.file_utils import cached_path\n",
        "from allennlp.data.fields import (\n",
        "    Field,\n",
        "    TextField,\n",
        "    MetadataField,\n",
        "    LabelField,\n",
        "    ListField,\n",
        "    SequenceLabelField,\n",
        "    SpanField,\n",
        "    IndexField,\n",
        ")\n",
        "\n",
        "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
        "from allennlp.data.instance import Instance\n",
        "from allennlp.data.token_indexers import SingleIdTokenIndexer, TokenIndexer\n",
        "from allennlp.data.tokenizers import Token, Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn4zlw2E2LaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# logging setting\n",
        "logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-t7DIUA3Tdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lv1Reader(DatasetReader):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: Tokenizer = None,\n",
        "        token_indexers: Dict[str, TokenIndexer] = None,\n",
        "        lazy: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__(lazy)\n",
        "        self._tokenizer = tokenizer or WhitespaceTokenizer()\n",
        "        self._token_indexers = token_indexers\n",
        "\n",
        "    @overrides\n",
        "    def _read(self, file_path: str):\n",
        "        file_path = cached_path(file_path)\n",
        "        \n",
        "        with codecs.open(file_path, mode='r', encoding='utf-8') as dataset_file:\n",
        "            for line in dataset_file:\n",
        "                \n",
        "                example = line.split('\\t')\n",
        "                payload = example[0]\n",
        "                payload = self._tokenizer.tokenize(payload)\n",
        "\n",
        "                target = None\n",
        "                if len(example) == 2:\n",
        "                    target = example[1].replace('\\n', '')\n",
        "                    target = int(target)\n",
        "                \n",
        "                instance = self.text_to_instance(\n",
        "                    payload,\n",
        "                    target,\n",
        "                )\n",
        "                yield instance\n",
        "                \n",
        "\n",
        "    @overrides\n",
        "    def text_to_instance(\n",
        "        self,  # type: ignore\n",
        "        payload: str,\n",
        "        target: int,\n",
        "        ) -> Union[Instance, None]:\n",
        "        \n",
        "        fields: Dict[str, Field] = {}\n",
        "        fields[\"payload\"] = TextField(payload, self._token_indexers)\n",
        "\n",
        "        if target is not None:\n",
        "            fields[\"target\"] = LabelField(target, skip_indexing=True)\n",
        "        \n",
        "        return Instance(fields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7thodi7J3Ta4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.data.tokenizers.character_tokenizer import CharacterTokenizer\n",
        "from allennlp.data.token_indexers.token_characters_indexer import TokenCharactersIndexer\n",
        "\n",
        "import os\n",
        "import codecs\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7gAlraO3TVZ",
        "colab_type": "code",
        "outputId": "8d10fbbf-dfb0-40db-e2f4-88c7f6129b59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "reader = Lv1Reader(tokenizer = CharacterTokenizer(),\n",
        "                  token_indexers = {\"token_characters\": TokenCharactersIndexer(min_padding_length = 5)}\n",
        "                  )\n",
        "\n",
        "#all_dataset = reader.read(TRAIN_PATH)\n",
        "all_dataset = reader.read(AUG_PATH)\n",
        "train_dataset, validation_dataset = train_test_split(all_dataset, test_size=0.2, random_state=11)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "600it [00:00, 3488.68it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5x-zVXn_mN3",
        "colab_type": "code",
        "outputId": "838385d9-f52f-42a7-e930-7747ead4a2dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.data.iterators import BasicIterator, BucketIterator\n",
        "\n",
        "vocab = Vocabulary.from_instances(train_dataset, min_count={'token_characters': 1})\n",
        "iterator = BucketIterator(\n",
        "    batch_size=2,\n",
        "    sorting_keys=[(\"payload\", \"num_tokens\")],\n",
        ")\n",
        "iterator.index_with(vocab)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting token dictionary from dataset.\n",
            "100%|██████████| 480/480 [00:00<00:00, 3880.93it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guo3R8nNzk_k",
        "colab_type": "code",
        "outputId": "8b374cfe-4173-434a-f802-51fdfdda3a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "from allennlp.common.params import Params\n",
        "\n",
        "char_embedding_params = Params({\n",
        "    'embedding': {\"embedding_dim\": 64},\n",
        "    'encoder': {\"type\": \"cnn\",\n",
        "                \"embedding_dim\": 64,\n",
        "                \"num_filters\": 100,\n",
        "                \"ngram_filter_sizes\": [2]\n",
        "               }\n",
        "})\n",
        "\n",
        "from allennlp.modules.token_embedders import Embedding,TokenCharactersEncoder\n",
        "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
        "\n",
        "char_embedding = TokenCharactersEncoder.from_params(vocab, char_embedding_params)\n",
        "embedder = BasicTextFieldEmbedder({\"token_characters\": char_embedding})"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding.num_embeddings = None\n",
            "embedding.vocab_namespace = token_characters\n",
            "embedding.embedding_dim = 64\n",
            "embedding.pretrained_file = None\n",
            "embedding.projection_dim = None\n",
            "embedding.trainable = True\n",
            "embedding.padding_index = None\n",
            "embedding.max_norm = None\n",
            "embedding.norm_type = 2.0\n",
            "embedding.scale_grad_by_freq = False\n",
            "embedding.sparse = False\n",
            "instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'type': 'cnn', 'embedding_dim': 64, 'num_filters': 100, 'ngram_filter_sizes': [2]} and extras set()\n",
            "encoder.type = cnn\n",
            "instantiating class <class 'allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder'> from params {'embedding_dim': 64, 'num_filters': 100, 'ngram_filter_sizes': [2]} and extras set()\n",
            "instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "encoder.embedding_dim = 64\n",
            "encoder.num_filters = 100\n",
            "encoder.ngram_filter_sizes = [2]\n",
            "encoder.output_dim = None\n",
            "instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "dropout = 0.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asYKv35WFAOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.nn.util import get_text_field_mask\n",
        "from typing import Dict, Optional\n",
        "\n",
        "from overrides import overrides\n",
        "import torch\n",
        "\n",
        "from allennlp.data import Vocabulary\n",
        "from allennlp.models.model import Model\n",
        "from allennlp.modules import FeedForward, Seq2SeqEncoder, Seq2VecEncoder, TextFieldEmbedder\n",
        "from allennlp.nn import InitializerApplicator, RegularizerApplicator\n",
        "from allennlp.nn.util import get_text_field_mask\n",
        "from allennlp.training.metrics import CategoricalAccuracy\n",
        "\n",
        "from allennlp.training.metrics.auc import Auc\n",
        "\n",
        "class Lv1Net(Model):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab: Vocabulary,\n",
        "        text_field_embedder: TextFieldEmbedder,\n",
        "        seq2vec_encoder: Seq2VecEncoder,\n",
        "        num_labels: int,\n",
        "        feedforward: Optional[FeedForward] = None,\n",
        "        dropout_prob: float = 0.1,\n",
        "        initializer: InitializerApplicator = InitializerApplicator(),\n",
        "        regularizer: Optional[RegularizerApplicator] = None,\n",
        "    ) -> None:\n",
        "        super().__init__(vocab, regularizer)\n",
        "\n",
        "        self._text_field_embedder = text_field_embedder\n",
        "        text_embed_dim = text_field_embedder.get_output_dim()\n",
        "        self._dropout = torch.nn.Dropout(p=dropout_prob)\n",
        "\n",
        "        self._seq2vec_encoder = seq2vec_encoder\n",
        "        self._feedforward = feedforward\n",
        "        self._num_labels = num_labels\n",
        "        \n",
        "        if feedforward is not None:\n",
        "            self._classifier_input_dim = self._feedforward.get_output_dim()\n",
        "        else:\n",
        "            self._classifier_input_dim = self._seq2vec_encoder.get_output_dim()\n",
        "\n",
        "        self._classification_layer = torch.nn.Linear(self._classifier_input_dim, self._num_labels)\n",
        "\n",
        "        self._accuracy = CategoricalAccuracy()\n",
        "        self._auc = Auc(positive_label = 1)\n",
        "        self._loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "        initializer(self)\n",
        "        \n",
        "\n",
        "    def forward(  # type: ignore\n",
        "        self,\n",
        "        payload: Dict[str, torch.LongTensor],\n",
        "        target: torch.IntTensor = None,\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        \n",
        "        mask = get_text_field_mask(payload).float()\n",
        "\n",
        "\n",
        "        # Shape: (batch_size, payload_length, hidden)\n",
        "        embedded_text = self._text_field_embedder(payload)\n",
        "        embedded_text = self._seq2vec_encoder(embedded_text, mask=mask)\n",
        "\n",
        "        embedded_text = self._dropout(embedded_text)\n",
        "\n",
        "        if self._feedforward is not None:\n",
        "            embedded_text = self._feedforward(embedded_text)\n",
        "\n",
        "        logits = self._classification_layer(embedded_text)\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "        output_dict = {\"logits\": logits, \"probs\": probs}\n",
        "\n",
        "        if target is not None:\n",
        "            loss = self._loss(logits, target.long().view(-1))\n",
        "            output_dict[\"loss\"] = loss\n",
        "            self._accuracy(logits, target)\n",
        "            self._auc(logits.argmax(dim=1), target)\n",
        "        \n",
        "        return output_dict\n",
        "\n",
        "    @overrides\n",
        "    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Does a simple argmax over the probabilities, converts index to string label, and\n",
        "        add ``\"label\"`` key to the dictionary with the result.\n",
        "        \"\"\"\n",
        "        predictions = output_dict[\"probs\"]\n",
        "        if predictions.dim() == 2:\n",
        "            predictions_list = [predictions[i] for i in range(predictions.shape[0])]\n",
        "        else:\n",
        "            predictions_list = [predictions]\n",
        "        classes = []\n",
        "        for prediction in predictions_list:\n",
        "            label_idx = prediction.argmax(dim=-1).item()\n",
        "            label_str = str(label_idx)\n",
        "            classes.append(label_str)\n",
        "        output_dict[\"label\"] = classes\n",
        "        return output_dict\n",
        "\n",
        "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "        #metrics = {}\n",
        "        metrics = {\"accuracy\": self._accuracy.get_metric(reset), \"auc\": self._auc.get_metric(reset)}\n",
        "        return metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WvuD2-5Kzgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.modules.seq2vec_encoders.pytorch_seq2vec_wrapper import PytorchSeq2VecWrapper\n",
        "from torch.nn import GRU\n",
        "\n",
        "rnn = GRU(input_size = 100, hidden_size = 64, batch_first = True, bidirectional = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-cNlfJtKzeG",
        "colab_type": "code",
        "outputId": "399b4167-ae81-4440-b5f2-09d49c8ac596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "model = Lv1Net(vocab = vocab,\n",
        "               text_field_embedder = embedder,\n",
        "               seq2vec_encoder = PytorchSeq2VecWrapper(rnn),\n",
        "               num_labels = 2,\n",
        "               dropout_prob = 0.1\n",
        "               )"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing parameters\n",
            "Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "   _classification_layer.bias\n",
            "   _classification_layer.weight\n",
            "   _seq2vec_encoder._module.bias_hh_l0\n",
            "   _seq2vec_encoder._module.bias_hh_l0_reverse\n",
            "   _seq2vec_encoder._module.bias_ih_l0\n",
            "   _seq2vec_encoder._module.bias_ih_l0_reverse\n",
            "   _seq2vec_encoder._module.weight_hh_l0\n",
            "   _seq2vec_encoder._module.weight_hh_l0_reverse\n",
            "   _seq2vec_encoder._module.weight_ih_l0\n",
            "   _seq2vec_encoder._module.weight_ih_l0_reverse\n",
            "   _text_field_embedder.token_embedder_token_characters._embedding._module.weight\n",
            "   _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias\n",
            "   _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDJcbWxGKzbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "LR = 1e-4\n",
        "#optimizer = optim.Adam(model.parameters(), lr=LR, betas = [0.8, 0.999], eps = 1e-7 )\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoBK_BBvKzY3",
        "colab_type": "code",
        "outputId": "3398451c-c7a0-4427-8c9f-e033c7f9bf0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"cuda\")\n",
        "    cuda_device = 0\n",
        "    model = model.cuda(cuda_device)\n",
        "else:\n",
        "    print(\"cpu\")\n",
        "    cuda_device = -1"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_KEhpW_KzWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.training.trainer import Trainer\n",
        "\n",
        "PATIENCE = 8\n",
        "EPOCHS = 100\n",
        "\n",
        "trainer = Trainer(model = model,\n",
        "                  optimizer = optimizer,\n",
        "                  iterator = iterator,\n",
        "                  train_dataset = train_dataset,\n",
        "                  validation_dataset = validation_dataset,\n",
        "                  patience = PATIENCE,\n",
        "                  validation_metric = \"+auc\",\n",
        "                  #validation_metric = \"+accuracy\",\n",
        "                  num_epochs = EPOCHS,\n",
        "                  cuda_device = cuda_device\n",
        "                 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZe7GAL-KzS8",
        "colab_type": "code",
        "outputId": "2760be32-3c58-4d6b-ae32-86a8433fb330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning training.\n",
            "Epoch 0/99\n",
            "Peak CPU memory usage MB: 2571.208\n",
            "GPU 0 memory usage MB: 717\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.6771, auc: 0.5000, loss: 0.6333 ||: 100%|██████████| 240/240 [00:04<00:00, 49.04it/s]\n",
            "Validating\n",
            "accuracy: 0.7500, auc: 0.5000, loss: 0.5507 ||: 100%|██████████| 60/60 [00:00<00:00, 128.81it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.500  |     0.500\n",
            "loss            |     0.633  |     0.551\n",
            "cpu_memory_MB   |  2571.208  |       N/A\n",
            "accuracy        |     0.677  |     0.750\n",
            "gpu_0_memory_MB |   717.000  |       N/A\n",
            "Epoch duration: 0:00:05.459949\n",
            "Estimated training time remaining: 0:09:00\n",
            "Epoch 1/99\n",
            "Peak CPU memory usage MB: 2597.884\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "accuracy: 0.6771, auc: 0.5000, loss: 0.5577 ||: 100%|██████████| 240/240 [00:04<00:00, 56.56it/s]\n",
            "Validating\n",
            "  0%|          | 0/60 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.7500, auc: 0.5000, loss: 0.4558 ||: 100%|██████████| 60/60 [00:00<00:00, 133.66it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.500  |     0.500\n",
            "loss            |     0.558  |     0.456\n",
            "cpu_memory_MB   |  2597.884  |       N/A\n",
            "accuracy        |     0.677  |     0.750\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.795213\n",
            "Estimated training time remaining: 0:08:22\n",
            "Epoch 2/99\n",
            "Peak CPU memory usage MB: 2598.68\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "accuracy: 0.7208, auc: 0.5711, loss: 0.4601 ||: 100%|██████████| 240/240 [00:04<00:00, 59.17it/s]\n",
            "Validating\n",
            "  0%|          | 0/60 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9000, auc: 0.8444, loss: 0.3757 ||: 100%|██████████| 60/60 [00:00<00:00, 129.87it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.571  |     0.844\n",
            "loss            |     0.460  |     0.376\n",
            "cpu_memory_MB   |  2598.680  |       N/A\n",
            "accuracy        |     0.721  |     0.900\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.761314\n",
            "Estimated training time remaining: 0:08:05\n",
            "Epoch 3/99\n",
            "Peak CPU memory usage MB: 2599.008\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9083, auc: 0.8935, loss: 0.3423 ||: 100%|██████████| 240/240 [00:04<00:00, 56.99it/s]\n",
            "Validating\n",
            "accuracy: 0.9000, auc: 0.8778, loss: 0.2722 ||: 100%|██████████| 60/60 [00:00<00:00, 134.43it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.893  |     0.878\n",
            "loss            |     0.342  |     0.272\n",
            "cpu_memory_MB   |  2599.008  |       N/A\n",
            "accuracy        |     0.908  |     0.900\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.761491\n",
            "Estimated training time remaining: 0:07:54\n",
            "Epoch 4/99\n",
            "Peak CPU memory usage MB: 2599.128\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "accuracy: 0.9208, auc: 0.9095, loss: 0.2022 ||: 100%|██████████| 240/240 [00:04<00:00, 57.03it/s]\n",
            "Validating\n",
            "  0%|          | 0/60 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9333, auc: 0.8889, loss: 0.1825 ||: 100%|██████████| 60/60 [00:00<00:00, 137.04it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.909  |     0.889\n",
            "loss            |     0.202  |     0.182\n",
            "cpu_memory_MB   |  2599.128  |       N/A\n",
            "accuracy        |     0.921  |     0.933\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.742078\n",
            "Estimated training time remaining: 0:07:46\n",
            "Epoch 5/99\n",
            "Peak CPU memory usage MB: 2599.264\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9542, auc: 0.9425, loss: 0.1209 ||: 100%|██████████| 240/240 [00:04<00:00, 56.99it/s]\n",
            "Validating\n",
            "accuracy: 0.9500, auc: 0.9222, loss: 0.1352 ||: 100%|██████████| 60/60 [00:00<00:00, 132.02it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.943  |     0.922\n",
            "loss            |     0.121  |     0.135\n",
            "cpu_memory_MB   |  2599.264  |       N/A\n",
            "accuracy        |     0.954  |     0.950\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.760814\n",
            "Estimated training time remaining: 0:07:39\n",
            "Epoch 6/99\n",
            "Peak CPU memory usage MB: 2599.364\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9771, auc: 0.9713, loss: 0.0758 ||: 100%|██████████| 240/240 [00:04<00:00, 52.48it/s]\n",
            "Validating\n",
            "accuracy: 0.9500, auc: 0.9444, loss: 0.1043 ||: 100%|██████████| 60/60 [00:00<00:00, 134.40it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.971  |     0.944\n",
            "loss            |     0.076  |     0.104\n",
            "cpu_memory_MB   |  2599.364  |       N/A\n",
            "accuracy        |     0.977  |     0.950\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.797205\n",
            "Estimated training time remaining: 0:07:33\n",
            "Epoch 7/99\n",
            "Peak CPU memory usage MB: 2599.488\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:113: RuntimeWarning: invalid value encountered in less\n",
            "  if np.any(dx < 0):\n",
            "accuracy: 0.9875, auc: 0.9857, loss: 0.0652 ||: 100%|██████████| 240/240 [00:04<00:00, 56.31it/s]\n",
            "Validating\n",
            "accuracy: 0.9750, auc: 0.9500, loss: 0.1129 ||: 100%|██████████| 60/60 [00:00<00:00, 134.83it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.986  |     0.950\n",
            "loss            |     0.065  |     0.113\n",
            "cpu_memory_MB   |  2599.488  |       N/A\n",
            "accuracy        |     0.988  |     0.975\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.800615\n",
            "Estimated training time remaining: 0:07:27\n",
            "Epoch 8/99\n",
            "Peak CPU memory usage MB: 2599.624\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "accuracy: 0.9875, auc: 0.9840, loss: 0.0457 ||: 100%|██████████| 240/240 [00:04<00:00, 56.80it/s]\n",
            "Validating\n",
            "  0%|          | 0/60 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9750, auc: 0.9611, loss: 0.0762 ||: 100%|██████████| 60/60 [00:00<00:00, 132.43it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.984  |     0.961\n",
            "loss            |     0.046  |     0.076\n",
            "cpu_memory_MB   |  2599.624  |       N/A\n",
            "accuracy        |     0.988  |     0.975\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.780056\n",
            "Estimated training time remaining: 0:07:21\n",
            "Epoch 9/99\n",
            "Peak CPU memory usage MB: 2599.752\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "accuracy: 0.9938, auc: 0.9920, loss: 0.0354 ||: 100%|██████████| 240/240 [00:04<00:00, 57.27it/s]\n",
            "Validating\n",
            "accuracy: 0.9750, auc: 0.9611, loss: 0.0613 ||: 100%|██████████| 60/60 [00:00<00:00, 133.29it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.992  |     0.961\n",
            "loss            |     0.035  |     0.061\n",
            "cpu_memory_MB   |  2599.752  |       N/A\n",
            "accuracy        |     0.994  |     0.975\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.741769\n",
            "Estimated training time remaining: 0:07:15\n",
            "Epoch 10/99\n",
            "Peak CPU memory usage MB: 2600.46\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9792, auc: 0.9711, loss: 0.0609 ||: 100%|██████████| 240/240 [00:04<00:00, 56.83it/s]\n",
            "Validating\n",
            "accuracy: 0.9833, auc: 0.9667, loss: 0.0627 ||: 100%|██████████| 60/60 [00:00<00:00, 137.30it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.971  |     0.967\n",
            "loss            |     0.061  |     0.063\n",
            "cpu_memory_MB   |  2600.460  |       N/A\n",
            "accuracy        |     0.979  |     0.983\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.760277\n",
            "Estimated training time remaining: 0:07:10\n",
            "Epoch 11/99\n",
            "Peak CPU memory usage MB: 2600.888\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9917, auc: 0.9888, loss: 0.0260 ||: 100%|██████████| 240/240 [00:04<00:00, 56.13it/s]\n",
            "Validating\n",
            "accuracy: 0.9917, auc: 0.9833, loss: 0.0424 ||: 100%|██████████| 60/60 [00:00<00:00, 129.89it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.989  |     0.983\n",
            "loss            |     0.026  |     0.042\n",
            "cpu_memory_MB   |  2600.888  |       N/A\n",
            "accuracy        |     0.992  |     0.992\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.833048\n",
            "Estimated training time remaining: 0:07:05\n",
            "Epoch 12/99\n",
            "Peak CPU memory usage MB: 2601.048\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9917, auc: 0.9905, loss: 0.0285 ||: 100%|██████████| 240/240 [00:04<00:00, 56.91it/s]\n",
            "Validating\n",
            "accuracy: 0.9833, auc: 0.9667, loss: 0.0504 ||: 100%|██████████| 60/60 [00:00<00:00, 132.88it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.990  |     0.967\n",
            "loss            |     0.028  |     0.050\n",
            "cpu_memory_MB   |  2601.048  |       N/A\n",
            "accuracy        |     0.992  |     0.983\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.841569\n",
            "Estimated training time remaining: 0:07:00\n",
            "Epoch 13/99\n",
            "Peak CPU memory usage MB: 2601.116\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9979, auc: 0.9968, loss: 0.0204 ||: 100%|██████████| 240/240 [00:04<00:00, 56.32it/s]\n",
            "Validating\n",
            "accuracy: 0.9833, auc: 0.9667, loss: 0.0525 ||: 100%|██████████| 60/60 [00:00<00:00, 134.43it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.997  |     0.967\n",
            "loss            |     0.020  |     0.052\n",
            "cpu_memory_MB   |  2601.116  |       N/A\n",
            "accuracy        |     0.998  |     0.983\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.801368\n",
            "Estimated training time remaining: 0:06:55\n",
            "Epoch 14/99\n",
            "Peak CPU memory usage MB: 2601.2\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "accuracy: 0.9979, auc: 0.9968, loss: 0.0176 ||: 100%|██████████| 240/240 [00:04<00:00, 56.26it/s]\n",
            "Validating\n",
            "  0%|          | 0/60 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9833, auc: 0.9667, loss: 0.0516 ||: 100%|██████████| 60/60 [00:00<00:00, 131.20it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.997  |     0.967\n",
            "loss            |     0.018  |     0.052\n",
            "cpu_memory_MB   |  2601.200  |       N/A\n",
            "accuracy        |     0.998  |     0.983\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.828645\n",
            "Estimated training time remaining: 0:06:50\n",
            "Epoch 15/99\n",
            "Peak CPU memory usage MB: 2601.232\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "accuracy: 0.9958, auc: 0.9935, loss: 0.0235 ||: 100%|██████████| 240/240 [00:04<00:00, 55.86it/s]\n",
            "Validating\n",
            "accuracy: 0.9917, auc: 0.9944, loss: 0.0505 ||: 100%|██████████| 60/60 [00:00<00:00, 130.44it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.994  |     0.994\n",
            "loss            |     0.023  |     0.050\n",
            "cpu_memory_MB   |  2601.232  |       N/A\n",
            "accuracy        |     0.996  |     0.992\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.853418\n",
            "Estimated training time remaining: 0:06:46\n",
            "Epoch 16/99\n",
            "Peak CPU memory usage MB: 2601.232\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "accuracy: 0.9958, auc: 0.9952, loss: 0.0189 ||: 100%|██████████| 240/240 [00:04<00:00, 56.25it/s]\n",
            "Validating\n",
            "  0%|          | 0/60 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9917, auc: 0.9833, loss: 0.0426 ||: 100%|██████████| 60/60 [00:00<00:00, 133.25it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.995  |     0.983\n",
            "loss            |     0.019  |     0.043\n",
            "cpu_memory_MB   |  2601.232  |       N/A\n",
            "accuracy        |     0.996  |     0.992\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.805804\n",
            "Estimated training time remaining: 0:06:41\n",
            "Epoch 17/99\n",
            "Peak CPU memory usage MB: 2601.232\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9979, auc: 0.9968, loss: 0.0147 ||: 100%|██████████| 240/240 [00:04<00:00, 56.21it/s]\n",
            "Validating\n",
            "accuracy: 0.9833, auc: 0.9778, loss: 0.0379 ||: 100%|██████████| 60/60 [00:00<00:00, 130.29it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.997  |     0.978\n",
            "loss            |     0.015  |     0.038\n",
            "cpu_memory_MB   |  2601.232  |       N/A\n",
            "accuracy        |     0.998  |     0.983\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.821016\n",
            "Estimated training time remaining: 0:06:36\n",
            "Epoch 18/99\n",
            "Peak CPU memory usage MB: 2601.252\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9979, auc: 0.9968, loss: 0.0110 ||: 100%|██████████| 240/240 [00:04<00:00, 56.81it/s]\n",
            "Validating\n",
            "accuracy: 0.9917, auc: 0.9833, loss: 0.0440 ||: 100%|██████████| 60/60 [00:00<00:00, 125.01it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.997  |     0.983\n",
            "loss            |     0.011  |     0.044\n",
            "cpu_memory_MB   |  2601.252  |       N/A\n",
            "accuracy        |     0.998  |     0.992\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.801502\n",
            "Estimated training time remaining: 0:06:31\n",
            "Epoch 19/99\n",
            "Peak CPU memory usage MB: 2601.32\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0104 ||: 100%|██████████| 240/240 [00:04<00:00, 55.68it/s]\n",
            "Validating\n",
            "accuracy: 0.9917, auc: 0.9944, loss: 0.0351 ||: 100%|██████████| 60/60 [00:00<00:00, 129.14it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     1.000  |     0.994\n",
            "loss            |     0.010  |     0.035\n",
            "cpu_memory_MB   |  2601.320  |       N/A\n",
            "accuracy        |     1.000  |     0.992\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.873522\n",
            "Estimated training time remaining: 0:06:26\n",
            "Epoch 20/99\n",
            "Peak CPU memory usage MB: 2601.42\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0088 ||: 100%|██████████| 240/240 [00:04<00:00, 54.30it/s]\n",
            "Validating\n",
            "  0%|          | 0/60 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9833, auc: 0.9778, loss: 0.0399 ||: 100%|██████████| 60/60 [00:00<00:00, 133.66it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     1.000  |     0.978\n",
            "loss            |     0.009  |     0.040\n",
            "cpu_memory_MB   |  2601.420  |       N/A\n",
            "accuracy        |     1.000  |     0.983\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.804774\n",
            "Estimated training time remaining: 0:06:21\n",
            "Epoch 21/99\n",
            "Peak CPU memory usage MB: 2601.468\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "accuracy: 0.9979, auc: 0.9968, loss: 0.0072 ||: 100%|██████████| 240/240 [00:04<00:00, 55.77it/s]\n",
            "Validating\n",
            "accuracy: 0.9917, auc: 0.9944, loss: 0.0272 ||: 100%|██████████| 60/60 [00:00<00:00, 136.69it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.997  |     0.994\n",
            "loss            |     0.007  |     0.027\n",
            "cpu_memory_MB   |  2601.468  |       N/A\n",
            "accuracy        |     0.998  |     0.992\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.800255\n",
            "Estimated training time remaining: 0:06:16\n",
            "Epoch 22/99\n",
            "Peak CPU memory usage MB: 2601.532\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9958, auc: 0.9935, loss: 0.0125 ||: 100%|██████████| 240/240 [00:04<00:00, 56.35it/s]\n",
            "Validating\n",
            "accuracy: 0.9917, auc: 0.9944, loss: 0.0347 ||: 100%|██████████| 60/60 [00:00<00:00, 131.07it/s]\n",
            "                    Training |  Validation\n",
            "auc             |     0.994  |     0.994\n",
            "loss            |     0.012  |     0.035\n",
            "cpu_memory_MB   |  2601.532  |       N/A\n",
            "accuracy        |     0.996  |     0.992\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "Epoch duration: 0:00:04.820776\n",
            "Estimated training time remaining: 0:06:12\n",
            "Epoch 23/99\n",
            "Peak CPU memory usage MB: 2601.584\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0069 ||: 100%|██████████| 240/240 [00:04<00:00, 56.52it/s]\n",
            "Validating\n",
            "accuracy: 0.9917, auc: 0.9833, loss: 0.0204 ||: 100%|██████████| 60/60 [00:00<00:00, 138.48it/s]\n",
            "Ran out of patience.  Stopping training.\n",
            "cannot load best weights without `serialization_dir`, so you're just getting the last weights\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best_epoch': 15,\n",
              " 'best_validation_accuracy': 0.9916666666666667,\n",
              " 'best_validation_auc': 0.9944444444444445,\n",
              " 'best_validation_loss': 0.05047054390112559,\n",
              " 'epoch': 22,\n",
              " 'peak_cpu_memory_MB': 2601.584,\n",
              " 'peak_gpu_0_memory_MB': 769,\n",
              " 'training_accuracy': 0.9958333333333333,\n",
              " 'training_auc': 0.9935483870967742,\n",
              " 'training_cpu_memory_MB': 2601.532,\n",
              " 'training_duration': '0:01:51.125785',\n",
              " 'training_epochs': 22,\n",
              " 'training_gpu_0_memory_MB': 769,\n",
              " 'training_loss': 0.012470971544583638,\n",
              " 'training_start_epoch': 0,\n",
              " 'validation_accuracy': 0.9916666666666667,\n",
              " 'validation_auc': 0.9944444444444445,\n",
              " 'validation_loss': 0.034675743679205576}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX1_Uct67zfU",
        "colab_type": "text"
      },
      "source": [
        "## Prediction (For Lv1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmmvtKLVKzQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "\n",
        "predictor = Predictor(model, reader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ncxwf2ONOl9",
        "colab_type": "code",
        "outputId": "1e15e1f5-70e1-4d96-ac44-9313799bb82d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "level1_pred = []\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "TEST_FILE = 'level1_test.tsv'\n",
        "TEST_PATH = os.path.join(DATA_DIR, TEST_FILE)\n",
        "test_dataset = reader.read(TEST_PATH)\n",
        "\n",
        "for instance in tqdm(test_dataset):\n",
        "    ans = predictor.predict_instance(instance)\n",
        "    level1_pred.append(ans['label'])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200it [00:00, 6285.11it/s]\n",
            "100%|██████████| 200/200 [00:01<00:00, 137.07it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mKQ-ARc3S9L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "2b1af9eb-2a3f-4434-97bc-4407084719b9"
      },
      "source": [
        "level1_pred_df = pd.DataFrame(level1_pred)\n",
        "level1_pred_df.T"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 200 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  0   1   2   3   4   5   6   7   8    ... 191 192 193 194 195 196 197 198 199\n",
              "0   1   1   1   1   1   0   1   0   1  ...   1   1   1   1   1   1   1   1   0\n",
              "\n",
              "[1 rows x 200 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W-V1q24lBgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "level1_pred_df.T.to_csv('level1_pred.csv', index = False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VWBm8RP3Sze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('level1_pred.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}