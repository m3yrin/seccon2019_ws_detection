{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seccon_ws_detection_m3yrin.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m3yrin/seccon2019_ws_detection/blob/master/seccon_ws_detection_m3yrin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1_5Busp3cYf",
        "colab_type": "text"
      },
      "source": [
        "# CNN model for 攻撃自動検知ワークショップ in SECCON 2019 \n",
        "\n",
        "https://connpass.com/event/159753/  \n",
        "seccon_ws_detection  \n",
        "\n",
        "auther : ＠m3yrin\n",
        "\n",
        "## Model Specification\n",
        "* Char-Base CNN sequence modeling\n",
        "\n",
        "## Memo\n",
        "* Bulit on AllenNLP\n",
        "* Tested on Google Colaboratory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za5eHvTQ4GbW",
        "colab_type": "text"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvdF94MkdMOq",
        "colab_type": "code",
        "outputId": "7102b460-b2db-43cd-b793-acb825247c3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone https://github.com/palloc/seccon_ws_detection.git\n",
        "!pip install allennlp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'seccon_ws_detection'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 19 (delta 1), reused 17 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (19/19), done.\n",
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Collecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 56.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/5c/e918d9f190baab8d55bad52840d8091dd5114cc99f03eaa6d72d404503cc/tensorboardX-1.9-py2.py3-none-any.whl (190kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 43.6MB/s \n",
            "\u001b[?25hCollecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Collecting numpydoc>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/f3/7cfe4c616e4b9fe05540256cc9c6661c052c8a4cec2915732793b36e1843/numpydoc-0.9.1.tar.gz\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.10.40)\n",
            "Collecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
            "Collecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 54.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.21.3)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.1)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.1.2)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/a6/e69e38f1f259fcf8532d8bd2c4bc88764f42d7b35a41423a7f4b035cc5ce/jsonnet-0.14.0.tar.gz (253kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 48.6MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ca/2d9a5030eaf1bcd925dab392762b9709a7ad4bd486a90599d93cd79cb188/ftfy-5.6.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.3)\n",
            "Collecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.17.4)\n",
            "Collecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/86/7a/532fc167366797f66c732549490dcf13243077f15446115f3c0ad17e56b8/overrides-2.6.tar.gz\n",
            "Collecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 47.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
            "Collecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
            "Collecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/3e/0c/940781dd49710f4b1f0650c450c9fd8491db0e1bffd99ebc36355607f96d/responses-0.10.9-py2.py3-none-any.whl\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 55.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp) (2019.12.9)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.4.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.3)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.9.6)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (7.0.8)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->allennlp) (1.12.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.0.2)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (42.0.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (2.10.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.13.40)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (0.16.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.14.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.6.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.7.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.15.2)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (19.2)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp) (1.1.1)\n",
            "Building wheels for collected packages: numpydoc, parsimonious, word2number, jsonnet, ftfy, overrides\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numpydoc: filename=numpydoc-0.9.1-cp36-none-any.whl size=31872 sha256=489249d973a86daf876f0b51813eecf519e5a17fe7406928d72552a7ec67cb73\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/30/d1/92a39ba40f21cb70e53f8af96eb98f002a781843c065406500\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42709 sha256=42e60c072cb26e3cb259d9b49e3544f82d513e6fda38523b213d6dcc39282ad8\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5588 sha256=1222ce77cac0c9a1183f65029f54710f31c2293912c9ade07b63c421d4b5e8c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.14.0-cp36-cp36m-linux_x86_64.whl size=3320352 sha256=fb7c6bb4f82e0eda8be7ca83ff33923998832484d2ed680c8b23ee5a44ad7bcc\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/b7/83/985f0f758fbb34f14989a0fab86d18890d1cc5ae12f26967bc\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.6-cp36-none-any.whl size=44553 sha256=3d457937a6ae68509fc848f0e2532c4da594e0cd0fe40f80eb7bb622635e0692\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/34/ce/cbb38d71543c408de56f3c5e26ce8ba495a0fa5a28eaaf1046\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.6-cp36-none-any.whl size=5523 sha256=2086abb9c82bc301ddf633df0cc85cca5a5b51912ae41e588a10ba84d76e94e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/86/49/c413319bcff638bdc13462c063c84d68e294d84514170c3744\n",
            "Successfully built numpydoc parsimonious word2number jsonnet ftfy overrides\n",
            "Installing collected packages: sentencepiece, pytorch-transformers, tensorboardX, conllu, numpydoc, parsimonious, word2number, unidecode, jsonpickle, jsonnet, ftfy, flaky, overrides, pytorch-pretrained-bert, flask-cors, responses, allennlp\n",
            "Successfully installed allennlp-0.9.0 conllu-1.3.1 flaky-3.6.1 flask-cors-3.0.8 ftfy-5.6 jsonnet-0.14.0 jsonpickle-1.2 numpydoc-0.9.1 overrides-2.6 parsimonious-0.8.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.10.9 sentencepiece-0.1.85 tensorboardX-1.9 unidecode-1.1.1 word2number-1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecNA1XeQdQmz",
        "colab_type": "code",
        "outputId": "a51572cb-df9a-4545-eb37-6962e16f030a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd seccon_ws_detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/seccon_ws_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK0ARpOC5akT",
        "colab_type": "code",
        "outputId": "39f735d2-bc77-43a1-a2d1-6ec2bd4b9c22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget -P dataset https://raw.githubusercontent.com/palloc/seccon_ws_detection/score/dataset/level2_train.tsv"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-21 16:15:18--  https://raw.githubusercontent.com/palloc/seccon_ws_detection/score/dataset/level2_train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12683 (12K) [text/plain]\n",
            "Saving to: ‘dataset/level2_train.tsv’\n",
            "\n",
            "\rlevel2_train.tsv      0%[                    ]       0  --.-KB/s               \rlevel2_train.tsv    100%[===================>]  12.39K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-12-21 16:15:18 (185 MB/s) - ‘dataset/level2_train.tsv’ saved [12683/12683]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3K4SJtw2rai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filepath configs\n",
        "import os\n",
        "\n",
        "full_path = './'\n",
        "DATA_DIR = os.path.join(full_path, 'dataset')\n",
        "\n",
        "#ORIGINAL_FILE = 'level1_train.tsv'\n",
        "ORIGINAL_FILE = 'level2_train.tsv'\n",
        "ORIGINAL_PATH = os.path.join(DATA_DIR, ORIGINAL_FILE)\n",
        "\n",
        "TRAIN_FILE = 'level2_split_train.tsv'\n",
        "TRAIN_PATH = os.path.join(DATA_DIR, TRAIN_FILE)\n",
        "\n",
        "VALID_FILE = 'level2_split_valid.tsv'\n",
        "VALID_PATH = os.path.join(DATA_DIR, VALID_FILE)\n",
        "\n",
        "AUG_FILE = 'train_aug.tsv'\n",
        "AUG_PATH = os.path.join(DATA_DIR, AUG_FILE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CHAcvy4LlCw",
        "colab_type": "text"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKReeNBevkdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import codecs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(seed=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3bwZ2YjLmrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load dataset.\n",
        "def load_dataset(path, num_augment = None):\n",
        "    dataset = []\n",
        "    X = []\n",
        "    y = []\n",
        "    with codecs.open(path, mode='r', encoding='utf-8') as fin:\n",
        "        dataset.extend(fin.readlines())\n",
        "    \n",
        "    for payload in dataset:\n",
        "        \n",
        "        \n",
        "        X.append(payload.split('\\t')[0])\n",
        "\n",
        "        label = payload.split('\\t')[1].replace('\\n', '')\n",
        "        label = int(label)\n",
        "        y.append(label)\n",
        "\n",
        "\n",
        "    # Data augmentation\n",
        "    # sampling random 2 indices and concatinating them.\n",
        "    if num_augment is not None:\n",
        "        X_aug = []\n",
        "        y_aug = []\n",
        "\n",
        "        ids_list = np.random.randint(len(X), size=(num_augment, 2))\n",
        "\n",
        "        for ids in ids_list:\n",
        "            X_concat = X[ids[0]] + X[ids[1]]\n",
        "\n",
        "            y_concat = 1 if int(y[ids[0]]) + int(y[ids[1]]) > 0 else 0\n",
        "\n",
        "            X_aug.append(X_concat)\n",
        "            y_aug.append(y_concat)\n",
        "        \n",
        "        # Also added original sequences\n",
        "        return  X_aug + X, y_aug + y\n",
        "\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxZXNriqLuBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = load_dataset(ORIGINAL_PATH)\n",
        "data_df = pd.DataFrame(zip(X, y))\n",
        "data_df.columns = ['X','y']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm7ieqj8wy8y",
        "colab_type": "code",
        "outputId": "0c677d6f-473a-44e8-81dd-b74008ff0988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data_df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: lipman@oasys.dt.navy.mil (Robert Lipman)</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Subject: CALL FOR PRESENTATIONS: Navy SciViz/V...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SPONSOR: NESS (Navy Engineering Software Syste...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This computer is occurred a error on the memory.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The purpose of the seminar is to present and e...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   X  y\n",
              "0     From: lipman@oasys.dt.navy.mil (Robert Lipman)  0\n",
              "1  Subject: CALL FOR PRESENTATIONS: Navy SciViz/V...  0\n",
              "2  SPONSOR: NESS (Navy Engineering Software Syste...  0\n",
              "3   This computer is occurred a error on the memory.  0\n",
              "4  The purpose of the seminar is to present and e...  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck9UYTm-wzvJ",
        "colab_type": "code",
        "outputId": "c3949fd7-44e8-45e2-a055-2cf1c46f6b74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "data_df.describe()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>200.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.501255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                y\n",
              "count  200.000000\n",
              "mean     0.500000\n",
              "std      0.501255\n",
              "min      0.000000\n",
              "25%      0.000000\n",
              "50%      0.500000\n",
              "75%      1.000000\n",
              "max      1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqTwbP9ZxjhA",
        "colab_type": "code",
        "outputId": "20f878c6-08cf-457b-d3b4-c5903cc72df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# check the distribution of y\n",
        "data_df.y.value_counts().plot(kind=\"bar\")\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD1CAYAAACrz7WZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nd0lqyLhLUkPGXZIa+h9/5BcFpHm8eAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VvlmfHDM7hx",
        "colab_type": "text"
      },
      "source": [
        "## File Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmXyFU2KNSEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.2, random_state=11)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urwIE_ZDNg4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.DataFrame(zip(train_X, train_y))\n",
        "valid_df = pd.DataFrame(zip(valid_X, valid_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i21L9xvN7Rv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.to_csv(TRAIN_PATH, sep='\\t' , index=False, header=False)\n",
        "valid_df.to_csv(VALID_PATH, sep='\\t' , index=False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifvlluJmyR-8",
        "colab_type": "text"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvAQdvmPwf-q",
        "colab_type": "code",
        "outputId": "26c7c091-a2d6-47e7-d7f5-29cc03a61805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "num_augment = 400\n",
        "X, y = load_dataset(TRAIN_PATH, num_augment = num_augment)\n",
        "\n",
        "data_auged_df = pd.DataFrame(zip(X, y))\n",
        "data_auged_df.columns = ['X','y']\n",
        "data_auged_df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>but he claims are due to improper maintenance ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a Japanese style drawing room #101\"&lt;div style=...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;/command&gt;enctype=rows=&lt;video&gt;&lt;source onerror=...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ComputerLand franchise and another retail comp...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"!#%&amp;)(*+,-./:;=?@]\\[^_'}|{~\"\"&lt;s&gt;\"\"&gt;&lt;script&gt;al...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   X  y\n",
              "0  but he claims are due to improper maintenance ...  1\n",
              "1  a Japanese style drawing room #101\"<div style=...  1\n",
              "2  </command>enctype=rows=<video><source onerror=...  1\n",
              "3  ComputerLand franchise and another retail comp...  1\n",
              "4  \"!#%&)(*+,-./:;=?@]\\[^_'}|{~\"\"<s>\"\"><script>al...  1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie7BlA_TwmS0",
        "colab_type": "code",
        "outputId": "9ece84c1-9b17-4bd4-c351-a26c488aac06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "data_auged_df.describe()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>560.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.710714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.453836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                y\n",
              "count  560.000000\n",
              "mean     0.710714\n",
              "std      0.453836\n",
              "min      0.000000\n",
              "25%      0.000000\n",
              "50%      1.000000\n",
              "75%      1.000000\n",
              "max      1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAv90JnDMBYj",
        "colab_type": "code",
        "outputId": "09536315-4727-4525-a36b-8c2520da5597",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "data_auged_df.y.value_counts().plot(kind=\"bar\")\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD1CAYAAACrz7WZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE0s6tDMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5mWEnxSMXNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_auged_df.to_csv(AUG_PATH, sep='\\t' , index = False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDKtqZjxMXKI",
        "colab_type": "code",
        "outputId": "5624085e-5bfe-44ec-9d9a-f558a35eaf8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls dataset"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "level1_test.tsv   level2_split_train.tsv  level2_train.tsv\n",
            "level1_train.tsv  level2_split_valid.tsv  train_aug.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoJTHRSIL_3h",
        "colab_type": "text"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7MZ71Vn2LdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "import itertools\n",
        "import json\n",
        "import logging\n",
        "import string\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Union, Tuple, Any\n",
        "\n",
        "from overrides import overrides\n",
        "from word2number.w2n import word_to_num\n",
        "\n",
        "import allennlp\n",
        "from allennlp.common.file_utils import cached_path\n",
        "from allennlp.data.fields import (\n",
        "    Field,\n",
        "    TextField,\n",
        "    MetadataField,\n",
        "    LabelField,\n",
        "    ListField,\n",
        "    SequenceLabelField,\n",
        "    SpanField,\n",
        "    IndexField,\n",
        ")\n",
        "\n",
        "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
        "from allennlp.data.instance import Instance\n",
        "from allennlp.data.token_indexers import SingleIdTokenIndexer, TokenIndexer\n",
        "from allennlp.data.tokenizers import Token, Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn4zlw2E2LaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# logging setting\n",
        "logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-t7DIUA3Tdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lv1Reader(DatasetReader):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: Tokenizer = None,\n",
        "        token_indexers: Dict[str, TokenIndexer] = None,\n",
        "        lazy: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__(lazy)\n",
        "        self._tokenizer = tokenizer or WhitespaceTokenizer()\n",
        "        self._token_indexers = token_indexers\n",
        "\n",
        "    @overrides\n",
        "    def _read(self, file_path: str):\n",
        "        file_path = cached_path(file_path)\n",
        "        \n",
        "        with codecs.open(file_path, mode='r', encoding='utf-8') as dataset_file:\n",
        "            for line in dataset_file:\n",
        "                \n",
        "                example = line.split('\\t')\n",
        "                payload = example[0]\n",
        "                payload = self._tokenizer.tokenize(payload)\n",
        "\n",
        "                target = None\n",
        "                if len(example) == 2:\n",
        "                    target = example[1].replace('\\n', '')\n",
        "                    target = int(target)\n",
        "                \n",
        "                instance = self.text_to_instance(\n",
        "                    payload,\n",
        "                    target,\n",
        "                )\n",
        "                yield instance\n",
        "                \n",
        "\n",
        "    @overrides\n",
        "    def text_to_instance(\n",
        "        self,  # type: ignore\n",
        "        payload: str,\n",
        "        target: int,\n",
        "        ) -> Union[Instance, None]:\n",
        "        \n",
        "        fields: Dict[str, Field] = {}\n",
        "        fields[\"payload\"] = TextField(payload, self._token_indexers)\n",
        "\n",
        "        if target is not None:\n",
        "            fields[\"target\"] = LabelField(target, skip_indexing=True)\n",
        "        \n",
        "        return Instance(fields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7thodi7J3Ta4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.data.tokenizers.character_tokenizer import CharacterTokenizer\n",
        "from allennlp.data.token_indexers.token_characters_indexer import TokenCharactersIndexer\n",
        "\n",
        "import os\n",
        "import codecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7gAlraO3TVZ",
        "colab_type": "code",
        "outputId": "e04b5bd4-6cce-41e4-9c8a-cb30a949ba3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "reader = Lv1Reader(tokenizer = CharacterTokenizer(),\n",
        "                  token_indexers = {\"token_characters\": TokenCharactersIndexer(min_padding_length = 2)}\n",
        "                  )\n",
        "\n",
        "train_dataset = reader.read(AUG_PATH)\n",
        "valid_dataset = reader.read(VALID_PATH)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "560it [00:00, 3845.93it/s]\n",
            "40it [00:00, 10010.27it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5x-zVXn_mN3",
        "colab_type": "code",
        "outputId": "ad01d1af-704d-4ca9-f329-5985eb63cd2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.data.iterators import BasicIterator, BucketIterator\n",
        "\n",
        "vocab = Vocabulary.from_instances(train_dataset, min_count={'token_characters': 1})\n",
        "iterator = BucketIterator(\n",
        "    batch_size=2,\n",
        "    sorting_keys=[(\"payload\", \"num_tokens\")],\n",
        ")\n",
        "iterator.index_with(vocab)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting token dictionary from dataset.\n",
            "100%|██████████| 560/560 [00:00<00:00, 4917.02it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guo3R8nNzk_k",
        "colab_type": "code",
        "outputId": "6cbca659-910b-49b8-a68d-3f8801b8cc29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "from allennlp.common.params import Params\n",
        "\n",
        "char_embedding_params = Params({\n",
        "    'embedding': {\"embedding_dim\": 64},\n",
        "    'encoder': {\"type\": \"cnn\",\n",
        "                \"embedding_dim\": 64,\n",
        "                \"num_filters\": 100,\n",
        "                \"ngram_filter_sizes\": [2]\n",
        "               }\n",
        "})\n",
        "\n",
        "from allennlp.modules.token_embedders import Embedding,TokenCharactersEncoder\n",
        "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
        "\n",
        "char_embedding = TokenCharactersEncoder.from_params(vocab, char_embedding_params)\n",
        "embedder = BasicTextFieldEmbedder({\"token_characters\": char_embedding})"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding.num_embeddings = None\n",
            "embedding.vocab_namespace = token_characters\n",
            "embedding.embedding_dim = 64\n",
            "embedding.pretrained_file = None\n",
            "embedding.projection_dim = None\n",
            "embedding.trainable = True\n",
            "embedding.padding_index = None\n",
            "embedding.max_norm = None\n",
            "embedding.norm_type = 2.0\n",
            "embedding.scale_grad_by_freq = False\n",
            "embedding.sparse = False\n",
            "instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'type': 'cnn', 'embedding_dim': 64, 'num_filters': 100, 'ngram_filter_sizes': [2]} and extras set()\n",
            "encoder.type = cnn\n",
            "instantiating class <class 'allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder'> from params {'embedding_dim': 64, 'num_filters': 100, 'ngram_filter_sizes': [2]} and extras set()\n",
            "instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "encoder.embedding_dim = 64\n",
            "encoder.num_filters = 100\n",
            "encoder.ngram_filter_sizes = [2]\n",
            "encoder.output_dim = None\n",
            "instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "dropout = 0.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asYKv35WFAOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.nn.util import get_text_field_mask\n",
        "from typing import Dict, Optional\n",
        "\n",
        "from overrides import overrides\n",
        "import torch\n",
        "\n",
        "from allennlp.data import Vocabulary\n",
        "from allennlp.models.model import Model\n",
        "from allennlp.modules import FeedForward, Seq2SeqEncoder, Seq2VecEncoder, TextFieldEmbedder\n",
        "from allennlp.nn import InitializerApplicator, RegularizerApplicator\n",
        "from allennlp.nn.util import get_text_field_mask\n",
        "from allennlp.training.metrics import CategoricalAccuracy\n",
        "\n",
        "from allennlp.training.metrics.auc import Auc\n",
        "\n",
        "class Lv1Net(Model):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab: Vocabulary,\n",
        "        text_field_embedder: TextFieldEmbedder,\n",
        "        seq2vec_encoder: Seq2VecEncoder,\n",
        "        num_labels: int,\n",
        "        feedforward: Optional[FeedForward] = None,\n",
        "        dropout_prob: float = 0.1,\n",
        "        initializer: InitializerApplicator = InitializerApplicator(),\n",
        "        regularizer: Optional[RegularizerApplicator] = None,\n",
        "    ) -> None:\n",
        "        super().__init__(vocab, regularizer)\n",
        "\n",
        "        self._text_field_embedder = text_field_embedder\n",
        "        text_embed_dim = text_field_embedder.get_output_dim()\n",
        "        self._dropout = torch.nn.Dropout(p=dropout_prob)\n",
        "\n",
        "        self._seq2vec_encoder = seq2vec_encoder\n",
        "        self._feedforward = feedforward\n",
        "        self._num_labels = num_labels\n",
        "        \n",
        "        if feedforward is not None:\n",
        "            self._classifier_input_dim = self._feedforward.get_output_dim()\n",
        "        else:\n",
        "            self._classifier_input_dim = self._seq2vec_encoder.get_output_dim()\n",
        "\n",
        "        self._classification_layer = torch.nn.Linear(self._classifier_input_dim, self._num_labels)\n",
        "\n",
        "        self._accuracy = CategoricalAccuracy()\n",
        "        self._auc = Auc(positive_label = 1)\n",
        "        self._loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "        initializer(self)\n",
        "        \n",
        "\n",
        "    def forward(  # type: ignore\n",
        "        self,\n",
        "        payload: Dict[str, torch.LongTensor],\n",
        "        target: torch.IntTensor = None,\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        \n",
        "        mask = get_text_field_mask(payload).float()\n",
        "\n",
        "\n",
        "        # Shape: (batch_size, payload_length, hidden)\n",
        "        embedded_text = self._text_field_embedder(payload)\n",
        "        embedded_text = self._seq2vec_encoder(embedded_text, mask=mask)\n",
        "\n",
        "        embedded_text = self._dropout(embedded_text)\n",
        "\n",
        "        if self._feedforward is not None:\n",
        "            embedded_text = self._feedforward(embedded_text)\n",
        "\n",
        "        logits = self._classification_layer(embedded_text)\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "        output_dict = {\"logits\": logits, \"probs\": probs}\n",
        "\n",
        "        if target is not None:\n",
        "            loss = self._loss(logits, target.long().view(-1))\n",
        "            output_dict[\"loss\"] = loss\n",
        "            self._accuracy(logits, target)\n",
        "            self._auc(logits.argmax(dim=1), target)\n",
        "        \n",
        "        return output_dict\n",
        "\n",
        "    @overrides\n",
        "    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Does a simple argmax over the probabilities, converts index to string label, and\n",
        "        add ``\"label\"`` key to the dictionary with the result.\n",
        "        \"\"\"\n",
        "        predictions = output_dict[\"probs\"]\n",
        "        if predictions.dim() == 2:\n",
        "            predictions_list = [predictions[i] for i in range(predictions.shape[0])]\n",
        "        else:\n",
        "            predictions_list = [predictions]\n",
        "        classes = []\n",
        "        for prediction in predictions_list:\n",
        "            label_idx = prediction.argmax(dim=-1).item()\n",
        "            label_str = str(label_idx)\n",
        "            classes.append(label_str)\n",
        "        output_dict[\"label\"] = classes\n",
        "        return output_dict\n",
        "\n",
        "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "        #metrics = {}\n",
        "        metrics = {\"accuracy\": self._accuracy.get_metric(reset), \"auc\": self._auc.get_metric(reset)}\n",
        "        return metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WvuD2-5Kzgl",
        "colab_type": "code",
        "outputId": "7103dc8e-8fbe-4671-c01a-6537ebec14e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#from allennlp.modules.seq2vec_encoders.pytorch_seq2vec_wrapper import PytorchSeq2VecWrapper\n",
        "#from torch.nn import GRU\n",
        "#rnn = GRU(input_size = 100, hidden_size = 64, batch_first = True, bidirectional = True)\n",
        "\n",
        "from allennlp.modules.seq2vec_encoders.cnn_encoder import CnnEncoder\n",
        "encoder = CnnEncoder(embedding_dim = 100, num_filters = 100)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-cNlfJtKzeG",
        "colab_type": "code",
        "outputId": "db088277-b035-4d16-df3e-aaee53770559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model = Lv1Net(vocab = vocab,\n",
        "               text_field_embedder = embedder,\n",
        "               seq2vec_encoder = encoder,\n",
        "               num_labels = 2,\n",
        "               dropout_prob = 0.1\n",
        "               )"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing parameters\n",
            "Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "   _classification_layer.bias\n",
            "   _classification_layer.weight\n",
            "   _seq2vec_encoder.conv_layer_0.bias\n",
            "   _seq2vec_encoder.conv_layer_0.weight\n",
            "   _seq2vec_encoder.conv_layer_1.bias\n",
            "   _seq2vec_encoder.conv_layer_1.weight\n",
            "   _seq2vec_encoder.conv_layer_2.bias\n",
            "   _seq2vec_encoder.conv_layer_2.weight\n",
            "   _seq2vec_encoder.conv_layer_3.bias\n",
            "   _seq2vec_encoder.conv_layer_3.weight\n",
            "   _text_field_embedder.token_embedder_token_characters._embedding._module.weight\n",
            "   _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias\n",
            "   _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDJcbWxGKzbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "LR = 1e-4\n",
        "#optimizer = optim.Adam(model.parameters(), lr=LR, betas = [0.8, 0.999], eps = 1e-7 )\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoBK_BBvKzY3",
        "colab_type": "code",
        "outputId": "2767f715-f881-474a-aa26-48a677441066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"cuda\")\n",
        "    cuda_device = 0\n",
        "    model = model.cuda(cuda_device)\n",
        "else:\n",
        "    print(\"cpu\")\n",
        "    cuda_device = -1"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_KEhpW_KzWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.training.trainer import Trainer\n",
        "\n",
        "PATIENCE = 8\n",
        "EPOCHS = 100\n",
        "\n",
        "trainer = Trainer(model = model,\n",
        "                  optimizer = optimizer,\n",
        "                  iterator = iterator,\n",
        "                  train_dataset = train_dataset,\n",
        "                  validation_dataset = valid_dataset,\n",
        "                  patience = PATIENCE,\n",
        "                  validation_metric = \"+auc\",\n",
        "                  #validation_metric = \"+accuracy\",\n",
        "                  num_epochs = EPOCHS,\n",
        "                  cuda_device = cuda_device\n",
        "                 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZe7GAL-KzS8",
        "colab_type": "code",
        "outputId": "ea9ef02b-3a8c-47e7-fe01-14bf399d760b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning training.\n",
            "Epoch 0/99\n",
            "Peak CPU memory usage MB: 2570.516\n",
            "GPU 0 memory usage MB: 713\n",
            "Training\n",
            "accuracy: 0.7054, auc: 0.5328, loss: 0.5860 ||: 100%|██████████| 280/280 [00:02<00:00, 114.03it/s]\n",
            "Validating\n",
            "accuracy: 0.3500, auc: 0.5000, loss: 0.8179 ||: 100%|██████████| 20/20 [00:00<00:00, 332.63it/s]\n",
            "                    Training |  Validation\n",
            "loss            |     0.586  |     0.818\n",
            "cpu_memory_MB   |  2570.516  |       N/A\n",
            "auc             |     0.533  |     0.500\n",
            "gpu_0_memory_MB |   713.000  |       N/A\n",
            "accuracy        |     0.705  |     0.350\n",
            "Epoch duration: 0:00:02.588672\n",
            "Estimated training time remaining: 0:04:16\n",
            "Epoch 1/99\n",
            "Peak CPU memory usage MB: 2594.144\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "  0%|          | 0/280 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.7554, auc: 0.5772, loss: 0.3924 ||: 100%|██████████| 280/280 [00:01<00:00, 155.32it/s]\n",
            "Validating\n",
            "accuracy: 0.7750, auc: 0.8269, loss: 0.4413 ||: 100%|██████████| 20/20 [00:00<00:00, 361.63it/s]\n",
            "                    Training |  Validation\n",
            "loss            |     0.392  |     0.441\n",
            "cpu_memory_MB   |  2594.144  |       N/A\n",
            "auc             |     0.577  |     0.827\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "accuracy        |     0.755  |     0.775\n",
            "Epoch duration: 0:00:01.925788\n",
            "Estimated training time remaining: 0:03:41\n",
            "Epoch 2/99\n",
            "Peak CPU memory usage MB: 2594.68\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "accuracy: 0.9929, auc: 0.9895, loss: 0.1080 ||: 100%|██████████| 280/280 [00:01<00:00, 155.24it/s]\n",
            "Validating\n",
            "accuracy: 0.9750, auc: 0.9808, loss: 0.1294 ||: 100%|██████████| 20/20 [00:00<00:00, 325.32it/s]\n",
            "                    Training |  Validation\n",
            "loss            |     0.108  |     0.129\n",
            "cpu_memory_MB   |  2594.680  |       N/A\n",
            "auc             |     0.989  |     0.981\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "accuracy        |     0.993  |     0.975\n",
            "Epoch duration: 0:00:01.949956\n",
            "Estimated training time remaining: 0:03:29\n",
            "Epoch 3/99\n",
            "Peak CPU memory usage MB: 2594.784\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "  0%|          | 0/280 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9946, auc: 0.9962, loss: 0.0369 ||: 100%|██████████| 280/280 [00:01<00:00, 154.47it/s]\n",
            "Validating\n",
            "accuracy: 0.9750, auc: 0.9808, loss: 0.1237 ||: 100%|██████████| 20/20 [00:00<00:00, 361.47it/s]\n",
            "                    Training |  Validation\n",
            "loss            |     0.037  |     0.124\n",
            "cpu_memory_MB   |  2594.784  |       N/A\n",
            "auc             |     0.996  |     0.981\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "accuracy        |     0.995  |     0.975\n",
            "Epoch duration: 0:00:01.940457\n",
            "Estimated training time remaining: 0:03:21\n",
            "Epoch 4/99\n",
            "Peak CPU memory usage MB: 2595.384\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "accuracy: 0.9946, auc: 0.9962, loss: 0.0224 ||: 100%|██████████| 280/280 [00:01<00:00, 159.26it/s]\n",
            "Validating\n",
            "accuracy: 0.9750, auc: 0.9808, loss: 0.1235 ||: 100%|██████████| 20/20 [00:00<00:00, 319.72it/s]\n",
            "                    Training |  Validation\n",
            "loss            |     0.022  |     0.124\n",
            "cpu_memory_MB   |  2595.384  |       N/A\n",
            "auc             |     0.996  |     0.981\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "accuracy        |     0.995  |     0.975\n",
            "Epoch duration: 0:00:01.893229\n",
            "Estimated training time remaining: 0:03:15\n",
            "Epoch 5/99\n",
            "Peak CPU memory usage MB: 2595.952\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "accuracy: 0.9964, auc: 0.9975, loss: 0.0154 ||: 100%|██████████| 280/280 [00:01<00:00, 149.38it/s]\n",
            "Validating\n",
            "accuracy: 0.9750, auc: 0.9808, loss: 0.1255 ||: 100%|██████████| 20/20 [00:00<00:00, 332.31it/s]\n",
            "                    Training |  Validation\n",
            "loss            |     0.015  |     0.125\n",
            "cpu_memory_MB   |  2595.952  |       N/A\n",
            "auc             |     0.997  |     0.981\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "accuracy        |     0.996  |     0.975\n",
            "Epoch duration: 0:00:02.011484\n",
            "Estimated training time remaining: 0:03:13\n",
            "Epoch 6/99\n",
            "Peak CPU memory usage MB: 2595.964\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "  0%|          | 0/280 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9982, auc: 0.9987, loss: 0.0115 ||: 100%|██████████| 280/280 [00:01<00:00, 151.66it/s]\n",
            "Validating\n",
            "accuracy: 0.9750, auc: 0.9808, loss: 0.1134 ||: 100%|██████████| 20/20 [00:00<00:00, 355.10it/s]\n",
            "                    Training |  Validation\n",
            "loss            |     0.012  |     0.113\n",
            "cpu_memory_MB   |  2595.964  |       N/A\n",
            "auc             |     0.999  |     0.981\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "accuracy        |     0.998  |     0.975\n",
            "Epoch duration: 0:00:01.979473\n",
            "Estimated training time remaining: 0:03:10\n",
            "Epoch 7/99\n",
            "Peak CPU memory usage MB: 2595.996\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "  0%|          | 0/280 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9982, auc: 0.9987, loss: 0.0083 ||: 100%|██████████| 280/280 [00:01<00:00, 148.52it/s]\n",
            "Validating\n",
            "accuracy: 0.9750, auc: 0.9808, loss: 0.1150 ||: 100%|██████████| 20/20 [00:00<00:00, 360.02it/s]\n",
            "                    Training |  Validation\n",
            "loss            |     0.008  |     0.115\n",
            "cpu_memory_MB   |  2595.996  |       N/A\n",
            "auc             |     0.999  |     0.981\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "accuracy        |     0.998  |     0.975\n",
            "Epoch duration: 0:00:02.014230\n",
            "Estimated training time remaining: 0:03:07\n",
            "Epoch 8/99\n",
            "Peak CPU memory usage MB: 2596.012\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "  0%|          | 0/280 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9982, auc: 0.9987, loss: 0.0066 ||: 100%|██████████| 280/280 [00:01<00:00, 150.69it/s]\n",
            "Validating\n",
            "accuracy: 0.9750, auc: 0.9808, loss: 0.1037 ||: 100%|██████████| 20/20 [00:00<00:00, 368.59it/s]\n",
            "                    Training |  Validation\n",
            "loss            |     0.007  |     0.104\n",
            "cpu_memory_MB   |  2596.012  |       N/A\n",
            "auc             |     0.999  |     0.981\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "accuracy        |     0.998  |     0.975\n",
            "Epoch duration: 0:00:01.994007\n",
            "Estimated training time remaining: 0:03:05\n",
            "Epoch 9/99\n",
            "Peak CPU memory usage MB: 2596.028\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "  0%|          | 0/280 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9982, auc: 0.9987, loss: 0.0046 ||: 100%|██████████| 280/280 [00:01<00:00, 155.37it/s]\n",
            "Validating\n",
            "accuracy: 0.9750, auc: 0.9808, loss: 0.1240 ||: 100%|██████████| 20/20 [00:00<00:00, 280.30it/s]\n",
            "                    Training |  Validation\n",
            "loss            |     0.005  |     0.124\n",
            "cpu_memory_MB   |  2596.028  |       N/A\n",
            "auc             |     0.999  |     0.981\n",
            "gpu_0_memory_MB |   769.000  |       N/A\n",
            "accuracy        |     0.998  |     0.975\n",
            "Epoch duration: 0:00:01.948868\n",
            "Estimated training time remaining: 0:03:02\n",
            "Epoch 10/99\n",
            "Peak CPU memory usage MB: 2596.032\n",
            "GPU 0 memory usage MB: 769\n",
            "Training\n",
            "  0%|          | 0/280 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0037 ||: 100%|██████████| 280/280 [00:01<00:00, 159.84it/s]\n",
            "Validating\n",
            "accuracy: 0.9750, auc: 0.9808, loss: 0.1022 ||: 100%|██████████| 20/20 [00:00<00:00, 276.18it/s]\n",
            "Ran out of patience.  Stopping training.\n",
            "cannot load best weights without `serialization_dir`, so you're just getting the last weights\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best_epoch': 2,\n",
              " 'best_validation_accuracy': 0.975,\n",
              " 'best_validation_auc': 0.9807692307692308,\n",
              " 'best_validation_loss': 0.12937733829021453,\n",
              " 'epoch': 9,\n",
              " 'peak_cpu_memory_MB': 2596.032,\n",
              " 'peak_gpu_0_memory_MB': 769,\n",
              " 'training_accuracy': 0.9982142857142857,\n",
              " 'training_auc': 0.9987437185929648,\n",
              " 'training_cpu_memory_MB': 2596.028,\n",
              " 'training_duration': '0:00:20.266337',\n",
              " 'training_epochs': 9,\n",
              " 'training_gpu_0_memory_MB': 769,\n",
              " 'training_loss': 0.0045888322272471015,\n",
              " 'training_start_epoch': 0,\n",
              " 'validation_accuracy': 0.975,\n",
              " 'validation_auc': 0.9807692307692308,\n",
              " 'validation_loss': 0.12396755218505859}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX1_Uct67zfU",
        "colab_type": "text"
      },
      "source": [
        "## Prediction (For Lv1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmmvtKLVKzQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "\n",
        "predictor = Predictor(model, reader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ncxwf2ONOl9",
        "colab_type": "code",
        "outputId": "fad8b5a4-be52-4fa7-a87e-2ddb05d3f3c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "level1_pred = []\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "TEST_FILE = 'level1_test.tsv'\n",
        "TEST_PATH = os.path.join(DATA_DIR, TEST_FILE)\n",
        "test_dataset = reader.read(TEST_PATH)\n",
        "\n",
        "for instance in tqdm(test_dataset):\n",
        "    ans = predictor.predict_instance(instance)\n",
        "    level1_pred.append(ans['label'])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200it [00:00, 7495.58it/s]\n",
            "100%|██████████| 200/200 [00:00<00:00, 324.15it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mKQ-ARc3S9L",
        "colab_type": "code",
        "outputId": "34db9bb6-a26b-45b7-b971-bfb5aad2cb68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "level1_pred_df = pd.DataFrame(level1_pred)\n",
        "level1_pred_df.T"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 200 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  0   1   2   3   4   5   6   7   8    ... 191 192 193 194 195 196 197 198 199\n",
              "0   1   0   1   1   1   0   1   0   1  ...   1   1   1   0   1   1   0   0   0\n",
              "\n",
              "[1 rows x 200 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W-V1q24lBgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "level1_pred_df.T.to_csv('level1_pred.csv', index = False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VWBm8RP3Sze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('level1_pred.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}