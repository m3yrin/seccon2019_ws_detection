{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seccon_ws_detection_m3yrin.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m3yrin/seccon2019_ws_detection/blob/master/seccon_ws_detection_m3yrin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1_5Busp3cYf",
        "colab_type": "text"
      },
      "source": [
        "# CNN model for 攻撃自動検知ワークショップ in SECCON 2019 \n",
        "\n",
        "https://connpass.com/event/159753/  \n",
        "seccon_ws_detection  \n",
        "\n",
        "auther : ＠m3yrin\n",
        "\n",
        "## Model Specification\n",
        "* Char-Base CNN sequence modeling\n",
        "\n",
        "## Memo\n",
        "* Bulit on AllenNLP\n",
        "* Tested on Google Colaboratory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za5eHvTQ4GbW",
        "colab_type": "text"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvdF94MkdMOq",
        "colab_type": "code",
        "outputId": "b50e2cf3-119b-40e1-a308-54ae3a6ea2f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone https://github.com/palloc/seccon_ws_detection.git\n",
        "!pip install allennlp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'seccon_ws_detection'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 19 (delta 1), reused 17 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (19/19), done.\n",
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 8.9MB/s \n",
            "\u001b[?25hCollecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ca/2d9a5030eaf1bcd925dab392762b9709a7ad4bd486a90599d93cd79cb188/ftfy-5.6.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.8MB/s \n",
            "\u001b[?25hCollecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/a6/e69e38f1f259fcf8532d8bd2c4bc88764f42d7b35a41423a7f4b035cc5ce/jsonnet-0.14.0.tar.gz (253kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 51.6MB/s \n",
            "\u001b[?25hCollecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 47.2MB/s \n",
            "\u001b[?25hCollecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.3)\n",
            "Collecting numpydoc>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/f3/7cfe4c616e4b9fe05540256cc9c6661c052c8a4cec2915732793b36e1843/numpydoc-0.9.1.tar.gz\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/5c/e918d9f190baab8d55bad52840d8091dd5114cc99f03eaa6d72d404503cc/tensorboardX-1.9-py2.py3-none-any.whl (190kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 53.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
            "Collecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 50.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.17.4)\n",
            "Collecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.21.3)\n",
            "Requirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Collecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Collecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/3e/0c/940781dd49710f4b1f0650c450c9fd8491db0e1bffd99ebc36355607f96d/responses-0.10.9-py2.py3-none-any.whl\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Collecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Collecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.10.40)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.1)\n",
            "Collecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 59.7MB/s \n",
            "\u001b[?25hCollecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/86/7a/532fc167366797f66c732549490dcf13243077f15446115f3c0ad17e56b8/overrides-2.6.tar.gz\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from parsimonious>=0.8.0->allennlp) (1.12.0)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (2.10.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.11.28)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (42.0.2)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.0.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp) (2019.12.9)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.6.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.14.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (7.0.8)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.9.6)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.4.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.13.40)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (0.16.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.2)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (19.2)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.7.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.15.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp) (1.1.1)\n",
            "Building wheels for collected packages: ftfy, jsonnet, parsimonious, numpydoc, word2number, overrides\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.6-cp36-none-any.whl size=44553 sha256=e5036b2f40111f203ae726ce9556e8883be262a051dad1a87d012f86ceec8be1\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/34/ce/cbb38d71543c408de56f3c5e26ce8ba495a0fa5a28eaaf1046\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.14.0-cp36-cp36m-linux_x86_64.whl size=3320380 sha256=8107b5992724f21bf2232d7f81f189abc5869ae28b5fdf31d89bfaae293d7a64\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/b7/83/985f0f758fbb34f14989a0fab86d18890d1cc5ae12f26967bc\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42709 sha256=2fd09cd263c5baa5ce6ac43a75499a76dcb6528a5ba65e7d10f721b003c726c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numpydoc: filename=numpydoc-0.9.1-cp36-none-any.whl size=31872 sha256=8495e1561f5f24acc7860008eea0e87fa00b0cc3f8cca0bee451683e5416d289\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/30/d1/92a39ba40f21cb70e53f8af96eb98f002a781843c065406500\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5588 sha256=5eb59c34d123c7f76a588a7667c87b8e284a169bbf76c8504c3a93083b622e40\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.6-cp36-none-any.whl size=5523 sha256=b9b1aa5e86c5e2623712818531b56161e5d312c202f8fc9331dddca08f66ecde\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/86/49/c413319bcff638bdc13462c063c84d68e294d84514170c3744\n",
            "Successfully built ftfy jsonnet parsimonious numpydoc word2number overrides\n",
            "Installing collected packages: jsonpickle, ftfy, jsonnet, unidecode, parsimonious, numpydoc, tensorboardX, pytorch-pretrained-bert, conllu, flask-cors, responses, word2number, flaky, sentencepiece, pytorch-transformers, overrides, allennlp\n",
            "Successfully installed allennlp-0.9.0 conllu-1.3.1 flaky-3.6.1 flask-cors-3.0.8 ftfy-5.6 jsonnet-0.14.0 jsonpickle-1.2 numpydoc-0.9.1 overrides-2.6 parsimonious-0.8.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.10.9 sentencepiece-0.1.85 tensorboardX-1.9 unidecode-1.1.1 word2number-1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecNA1XeQdQmz",
        "colab_type": "code",
        "outputId": "6b3f97fa-39bd-42fa-9f66-83d8c8a11a8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd seccon_ws_detection"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/seccon_ws_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK0ARpOC5akT",
        "colab_type": "code",
        "outputId": "ee22e42a-d3a0-4abe-9b58-e964745d4a97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget -P dataset https://raw.githubusercontent.com/palloc/seccon_ws_detection/score/dataset/level2_train.tsv"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-21 07:13:17--  https://raw.githubusercontent.com/palloc/seccon_ws_detection/score/dataset/level2_train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12683 (12K) [text/plain]\n",
            "Saving to: ‘dataset/level2_train.tsv’\n",
            "\n",
            "\rlevel2_train.tsv      0%[                    ]       0  --.-KB/s               \rlevel2_train.tsv    100%[===================>]  12.39K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-12-21 07:13:17 (202 MB/s) - ‘dataset/level2_train.tsv’ saved [12683/12683]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3K4SJtw2rai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filepath configs\n",
        "import os\n",
        "\n",
        "full_path = './'\n",
        "DATA_DIR = os.path.join(full_path, 'dataset')\n",
        "\n",
        "#TRAIN_FILE = 'level1_train.tsv'\n",
        "TRAIN_FILE = 'level2_train.tsv'\n",
        "TRAIN_PATH = os.path.join(DATA_DIR, TRAIN_FILE)\n",
        "\n",
        "AUG_FILE = 'train_aug.tsv'\n",
        "AUG_PATH = os.path.join(DATA_DIR, AUG_FILE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CHAcvy4LlCw",
        "colab_type": "text"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKReeNBevkdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import codecs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(seed=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3bwZ2YjLmrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load dataset.\n",
        "def load_dataset(path, num_augment = None):\n",
        "    dataset = []\n",
        "    X = []\n",
        "    y = []\n",
        "    with codecs.open(path, mode='r', encoding='utf-8') as fin:\n",
        "        dataset.extend(fin.readlines())\n",
        "    \n",
        "    for payload in dataset:\n",
        "        \n",
        "        \n",
        "        X.append(payload.split('\\t')[0])\n",
        "\n",
        "        label = payload.split('\\t')[1].replace('\\n', '')\n",
        "        label = int(label)\n",
        "        y.append(label)\n",
        "\n",
        "\n",
        "    # Data augmentation\n",
        "    # sampling random 2 indices and concatinating them.\n",
        "    if num_augment is not None:\n",
        "        X_aug = []\n",
        "        y_aug = []\n",
        "\n",
        "        ids_list = np.random.randint(len(X), size=(num_augment, 2))\n",
        "\n",
        "        for ids in ids_list:\n",
        "            X_concat = X[ids[0]] + X[ids[1]]\n",
        "\n",
        "            y_concat = 1 if int(y[ids[0]]) + int(y[ids[1]]) > 0 else 0\n",
        "\n",
        "            X_aug.append(X_concat)\n",
        "            y_aug.append(y_concat)\n",
        "        \n",
        "        # Also added original sequences\n",
        "        return  X_aug + X, y_aug + y\n",
        "\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxZXNriqLuBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = load_dataset(TRAIN_PATH)\n",
        "data_df = pd.DataFrame(zip(X, y))\n",
        "data_df.columns = ['X','y']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm7ieqj8wy8y",
        "colab_type": "code",
        "outputId": "b749d137-b1d6-4b91-f1f6-9ee0416e106d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data_df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: lipman@oasys.dt.navy.mil (Robert Lipman)</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Subject: CALL FOR PRESENTATIONS: Navy SciViz/V...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SPONSOR: NESS (Navy Engineering Software Syste...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This computer is occurred a error on the memory.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The purpose of the seminar is to present and e...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   X  y\n",
              "0     From: lipman@oasys.dt.navy.mil (Robert Lipman)  0\n",
              "1  Subject: CALL FOR PRESENTATIONS: Navy SciViz/V...  0\n",
              "2  SPONSOR: NESS (Navy Engineering Software Syste...  0\n",
              "3   This computer is occurred a error on the memory.  0\n",
              "4  The purpose of the seminar is to present and e...  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck9UYTm-wzvJ",
        "colab_type": "code",
        "outputId": "9374fac1-b053-467c-9b1b-b8d9dc438ac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "data_df.describe()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>200.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.501255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                y\n",
              "count  200.000000\n",
              "mean     0.500000\n",
              "std      0.501255\n",
              "min      0.000000\n",
              "25%      0.000000\n",
              "50%      0.500000\n",
              "75%      1.000000\n",
              "max      1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqTwbP9ZxjhA",
        "colab_type": "code",
        "outputId": "164be491-84e0-43fb-e77f-de7d490724b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# check the distribution of y\n",
        "data_df.y.value_counts().plot(kind=\"bar\")\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD1CAYAAACrz7WZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nd0lqyLhLUkPGXZIa+h9/5BcFpHm8eAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifvlluJmyR-8",
        "colab_type": "text"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvAQdvmPwf-q",
        "colab_type": "code",
        "outputId": "2f081c9e-9ae8-4248-f71e-bb2f1d46c579",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "num_augment = 400\n",
        "X, y = load_dataset(TRAIN_PATH, num_augment = num_augment)\n",
        "\n",
        "data_auged_df = pd.DataFrame(zip(X, y))\n",
        "data_auged_df.columns = ['X','y']\n",
        "data_auged_df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&gt;&gt;technical support over the phone, free softw...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>to select something necessary to make a necess...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;svg&gt;&lt;style&gt;&lt;img/src=x onerror=alert(1)// &lt;/b&gt;...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;/iframe&gt; /&gt;&lt;/textarea&gt;&lt;video&gt;&lt;source onerror=...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[ I'm thinking of making this post bi-weekly. ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   X  y\n",
              "0  >>technical support over the phone, free softw...  1\n",
              "1  to select something necessary to make a necess...  1\n",
              "2  <svg><style><img/src=x onerror=alert(1)// </b>...  1\n",
              "3  </iframe> /></textarea><video><source onerror=...  1\n",
              "4  [ I'm thinking of making this post bi-weekly. ...  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie7BlA_TwmS0",
        "colab_type": "code",
        "outputId": "ebbf57fd-f322-460d-9e18-049233ad98ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "data_auged_df.describe()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>600.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.691667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.462190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                y\n",
              "count  600.000000\n",
              "mean     0.691667\n",
              "std      0.462190\n",
              "min      0.000000\n",
              "25%      0.000000\n",
              "50%      1.000000\n",
              "75%      1.000000\n",
              "max      1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAv90JnDMBYj",
        "colab_type": "code",
        "outputId": "ca42adb3-c071-4dd0-fdb0-a777ea6027ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "data_auged_df.y.value_counts().plot(kind=\"bar\")\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD1CAYAAACrz7WZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEWcaUgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5mWEnxSMXNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_auged_df.to_csv(AUG_PATH, sep='\\t' , index = False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDKtqZjxMXKI",
        "colab_type": "code",
        "outputId": "f7131d3d-a351-4699-f458-09d65ac9ec3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls dataset"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "level1_test.tsv  level1_train.tsv  level2_train.tsv  train_aug.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoJTHRSIL_3h",
        "colab_type": "text"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7MZ71Vn2LdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import logging\n",
        "\n",
        "import itertools\n",
        "import json\n",
        "import logging\n",
        "import string\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Union, Tuple, Any\n",
        "\n",
        "from overrides import overrides\n",
        "from word2number.w2n import word_to_num\n",
        "\n",
        "import allennlp\n",
        "from allennlp.common.file_utils import cached_path\n",
        "from allennlp.data.fields import (\n",
        "    Field,\n",
        "    TextField,\n",
        "    MetadataField,\n",
        "    LabelField,\n",
        "    ListField,\n",
        "    SequenceLabelField,\n",
        "    SpanField,\n",
        "    IndexField,\n",
        ")\n",
        "\n",
        "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
        "from allennlp.data.instance import Instance\n",
        "from allennlp.data.token_indexers import SingleIdTokenIndexer, TokenIndexer\n",
        "from allennlp.data.tokenizers import Token, Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn4zlw2E2LaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# logging setting\n",
        "logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-t7DIUA3Tdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lv1Reader(DatasetReader):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: Tokenizer = None,\n",
        "        token_indexers: Dict[str, TokenIndexer] = None,\n",
        "        lazy: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__(lazy)\n",
        "        self._tokenizer = tokenizer or WhitespaceTokenizer()\n",
        "        self._token_indexers = token_indexers\n",
        "\n",
        "    @overrides\n",
        "    def _read(self, file_path: str):\n",
        "        file_path = cached_path(file_path)\n",
        "        \n",
        "        with codecs.open(file_path, mode='r', encoding='utf-8') as dataset_file:\n",
        "            for line in dataset_file:\n",
        "                \n",
        "                example = line.split('\\t')\n",
        "                payload = example[0]\n",
        "                payload = self._tokenizer.tokenize(payload)\n",
        "\n",
        "                target = None\n",
        "                if len(example) == 2:\n",
        "                    target = example[1].replace('\\n', '')\n",
        "                    target = int(target)\n",
        "                \n",
        "                instance = self.text_to_instance(\n",
        "                    payload,\n",
        "                    target,\n",
        "                )\n",
        "                yield instance\n",
        "                \n",
        "\n",
        "    @overrides\n",
        "    def text_to_instance(\n",
        "        self,  # type: ignore\n",
        "        payload: str,\n",
        "        target: int,\n",
        "        ) -> Union[Instance, None]:\n",
        "        \n",
        "        fields: Dict[str, Field] = {}\n",
        "        fields[\"payload\"] = TextField(payload, self._token_indexers)\n",
        "\n",
        "        if target is not None:\n",
        "            fields[\"target\"] = LabelField(target, skip_indexing=True)\n",
        "        \n",
        "        return Instance(fields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7thodi7J3Ta4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.data.tokenizers.character_tokenizer import CharacterTokenizer\n",
        "from allennlp.data.token_indexers.token_characters_indexer import TokenCharactersIndexer\n",
        "\n",
        "import os\n",
        "import codecs\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7gAlraO3TVZ",
        "colab_type": "code",
        "outputId": "c2732da6-2102-4cd8-8ebc-0e1c31e2c9b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "reader = Lv1Reader(tokenizer = CharacterTokenizer(),\n",
        "                  token_indexers = {\"token_characters\": TokenCharactersIndexer(min_padding_length = 2)}\n",
        "                  )\n",
        "\n",
        "#all_dataset = reader.read(TRAIN_PATH)\n",
        "all_dataset = reader.read(AUG_PATH)\n",
        "train_dataset, validation_dataset = train_test_split(all_dataset, test_size=0.2, random_state=11)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "600it [00:00, 4206.24it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5x-zVXn_mN3",
        "colab_type": "code",
        "outputId": "646052f6-bbec-4a0f-b1de-d35f236c60e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.data.iterators import BasicIterator, BucketIterator\n",
        "\n",
        "vocab = Vocabulary.from_instances(train_dataset, min_count={'token_characters': 1})\n",
        "iterator = BucketIterator(\n",
        "    batch_size=2,\n",
        "    sorting_keys=[(\"payload\", \"num_tokens\")],\n",
        ")\n",
        "iterator.index_with(vocab)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting token dictionary from dataset.\n",
            "100%|██████████| 480/480 [00:00<00:00, 5356.43it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guo3R8nNzk_k",
        "colab_type": "code",
        "outputId": "9b5531a9-59d9-42f4-8f99-534a84992582",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "from allennlp.common.params import Params\n",
        "\n",
        "char_embedding_params = Params({\n",
        "    'embedding': {\"embedding_dim\": 64},\n",
        "    'encoder': {\"type\": \"cnn\",\n",
        "                \"embedding_dim\": 64,\n",
        "                \"num_filters\": 100,\n",
        "                \"ngram_filter_sizes\": [2]\n",
        "               }\n",
        "})\n",
        "\n",
        "from allennlp.modules.token_embedders import Embedding,TokenCharactersEncoder\n",
        "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
        "\n",
        "char_embedding = TokenCharactersEncoder.from_params(vocab, char_embedding_params)\n",
        "embedder = BasicTextFieldEmbedder({\"token_characters\": char_embedding})"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding.num_embeddings = None\n",
            "embedding.vocab_namespace = token_characters\n",
            "embedding.embedding_dim = 64\n",
            "embedding.pretrained_file = None\n",
            "embedding.projection_dim = None\n",
            "embedding.trainable = True\n",
            "embedding.padding_index = None\n",
            "embedding.max_norm = None\n",
            "embedding.norm_type = 2.0\n",
            "embedding.scale_grad_by_freq = False\n",
            "embedding.sparse = False\n",
            "instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'type': 'cnn', 'embedding_dim': 64, 'num_filters': 100, 'ngram_filter_sizes': [2]} and extras set()\n",
            "encoder.type = cnn\n",
            "instantiating class <class 'allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder'> from params {'embedding_dim': 64, 'num_filters': 100, 'ngram_filter_sizes': [2]} and extras set()\n",
            "instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "encoder.embedding_dim = 64\n",
            "encoder.num_filters = 100\n",
            "encoder.ngram_filter_sizes = [2]\n",
            "encoder.output_dim = None\n",
            "instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "dropout = 0.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asYKv35WFAOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.nn.util import get_text_field_mask\n",
        "from typing import Dict, Optional\n",
        "\n",
        "from overrides import overrides\n",
        "import torch\n",
        "\n",
        "from allennlp.data import Vocabulary\n",
        "from allennlp.models.model import Model\n",
        "from allennlp.modules import FeedForward, Seq2SeqEncoder, Seq2VecEncoder, TextFieldEmbedder\n",
        "from allennlp.nn import InitializerApplicator, RegularizerApplicator\n",
        "from allennlp.nn.util import get_text_field_mask\n",
        "from allennlp.training.metrics import CategoricalAccuracy\n",
        "\n",
        "from allennlp.training.metrics.auc import Auc\n",
        "\n",
        "class Lv1Net(Model):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab: Vocabulary,\n",
        "        text_field_embedder: TextFieldEmbedder,\n",
        "        seq2vec_encoder: Seq2VecEncoder,\n",
        "        num_labels: int,\n",
        "        feedforward: Optional[FeedForward] = None,\n",
        "        dropout_prob: float = 0.1,\n",
        "        initializer: InitializerApplicator = InitializerApplicator(),\n",
        "        regularizer: Optional[RegularizerApplicator] = None,\n",
        "    ) -> None:\n",
        "        super().__init__(vocab, regularizer)\n",
        "\n",
        "        self._text_field_embedder = text_field_embedder\n",
        "        text_embed_dim = text_field_embedder.get_output_dim()\n",
        "        self._dropout = torch.nn.Dropout(p=dropout_prob)\n",
        "\n",
        "        self._seq2vec_encoder = seq2vec_encoder\n",
        "        self._feedforward = feedforward\n",
        "        self._num_labels = num_labels\n",
        "        \n",
        "        if feedforward is not None:\n",
        "            self._classifier_input_dim = self._feedforward.get_output_dim()\n",
        "        else:\n",
        "            self._classifier_input_dim = self._seq2vec_encoder.get_output_dim()\n",
        "\n",
        "        self._classification_layer = torch.nn.Linear(self._classifier_input_dim, self._num_labels)\n",
        "\n",
        "        self._accuracy = CategoricalAccuracy()\n",
        "        self._auc = Auc(positive_label = 1)\n",
        "        self._loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "        initializer(self)\n",
        "        \n",
        "\n",
        "    def forward(  # type: ignore\n",
        "        self,\n",
        "        payload: Dict[str, torch.LongTensor],\n",
        "        target: torch.IntTensor = None,\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        \n",
        "        mask = get_text_field_mask(payload).float()\n",
        "\n",
        "\n",
        "        # Shape: (batch_size, payload_length, hidden)\n",
        "        embedded_text = self._text_field_embedder(payload)\n",
        "        embedded_text = self._seq2vec_encoder(embedded_text, mask=mask)\n",
        "\n",
        "        embedded_text = self._dropout(embedded_text)\n",
        "\n",
        "        if self._feedforward is not None:\n",
        "            embedded_text = self._feedforward(embedded_text)\n",
        "\n",
        "        logits = self._classification_layer(embedded_text)\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "        output_dict = {\"logits\": logits, \"probs\": probs}\n",
        "\n",
        "        if target is not None:\n",
        "            loss = self._loss(logits, target.long().view(-1))\n",
        "            output_dict[\"loss\"] = loss\n",
        "            self._accuracy(logits, target)\n",
        "            self._auc(logits.argmax(dim=1), target)\n",
        "        \n",
        "        return output_dict\n",
        "\n",
        "    @overrides\n",
        "    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Does a simple argmax over the probabilities, converts index to string label, and\n",
        "        add ``\"label\"`` key to the dictionary with the result.\n",
        "        \"\"\"\n",
        "        predictions = output_dict[\"probs\"]\n",
        "        if predictions.dim() == 2:\n",
        "            predictions_list = [predictions[i] for i in range(predictions.shape[0])]\n",
        "        else:\n",
        "            predictions_list = [predictions]\n",
        "        classes = []\n",
        "        for prediction in predictions_list:\n",
        "            label_idx = prediction.argmax(dim=-1).item()\n",
        "            label_str = str(label_idx)\n",
        "            classes.append(label_str)\n",
        "        output_dict[\"label\"] = classes\n",
        "        return output_dict\n",
        "\n",
        "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "        #metrics = {}\n",
        "        metrics = {\"accuracy\": self._accuracy.get_metric(reset), \"auc\": self._auc.get_metric(reset)}\n",
        "        return metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WvuD2-5Kzgl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dc576a83-b03d-4f21-a77d-f4b86e013d9e"
      },
      "source": [
        "#from allennlp.modules.seq2vec_encoders.pytorch_seq2vec_wrapper import PytorchSeq2VecWrapper\n",
        "#from torch.nn import GRU\n",
        "#rnn = GRU(input_size = 100, hidden_size = 64, batch_first = True, bidirectional = True)\n",
        "\n",
        "from allennlp.modules.seq2vec_encoders.cnn_encoder import CnnEncoder\n",
        "encoder = CnnEncoder(embedding_dim = 100, num_filters = 100)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-cNlfJtKzeG",
        "colab_type": "code",
        "outputId": "24c6fc2f-b73a-423f-b048-683bae421c4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model = Lv1Net(vocab = vocab,\n",
        "               text_field_embedder = embedder,\n",
        "               #seq2vec_encoder = PytorchSeq2VecWrapper(rnn),\n",
        "               seq2vec_encoder = encoder,\n",
        "               num_labels = 2,\n",
        "               dropout_prob = 0.1\n",
        "               )"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing parameters\n",
            "Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "   _classification_layer.bias\n",
            "   _classification_layer.weight\n",
            "   _seq2vec_encoder.conv_layer_0.bias\n",
            "   _seq2vec_encoder.conv_layer_0.weight\n",
            "   _seq2vec_encoder.conv_layer_1.bias\n",
            "   _seq2vec_encoder.conv_layer_1.weight\n",
            "   _seq2vec_encoder.conv_layer_2.bias\n",
            "   _seq2vec_encoder.conv_layer_2.weight\n",
            "   _seq2vec_encoder.conv_layer_3.bias\n",
            "   _seq2vec_encoder.conv_layer_3.weight\n",
            "   _text_field_embedder.token_embedder_token_characters._embedding._module.weight\n",
            "   _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias\n",
            "   _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDJcbWxGKzbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "LR = 1e-4\n",
        "#optimizer = optim.Adam(model.parameters(), lr=LR, betas = [0.8, 0.999], eps = 1e-7 )\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoBK_BBvKzY3",
        "colab_type": "code",
        "outputId": "94292492-3adf-4a69-9fcd-4dbdf3275419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"cuda\")\n",
        "    cuda_device = 0\n",
        "    model = model.cuda(cuda_device)\n",
        "else:\n",
        "    print(\"cpu\")\n",
        "    cuda_device = -1"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_KEhpW_KzWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.training.trainer import Trainer\n",
        "\n",
        "PATIENCE = 8\n",
        "EPOCHS = 100\n",
        "\n",
        "trainer = Trainer(model = model,\n",
        "                  optimizer = optimizer,\n",
        "                  iterator = iterator,\n",
        "                  train_dataset = train_dataset,\n",
        "                  validation_dataset = validation_dataset,\n",
        "                  patience = PATIENCE,\n",
        "                  validation_metric = \"+auc\",\n",
        "                  #validation_metric = \"+accuracy\",\n",
        "                  num_epochs = EPOCHS,\n",
        "                  cuda_device = cuda_device\n",
        "                 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZe7GAL-KzS8",
        "colab_type": "code",
        "outputId": "4894fc60-62bc-4e20-acb0-b0200f2a435a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning training.\n",
            "Epoch 0/99\n",
            "Peak CPU memory usage MB: 2574.592\n",
            "GPU 0 memory usage MB: 713\n",
            "Training\n",
            "accuracy: 0.6500, auc: 0.4901, loss: 0.6219 ||: 100%|██████████| 240/240 [00:01<00:00, 131.38it/s]\n",
            "Validating\n",
            "accuracy: 0.7500, auc: 0.5000, loss: 0.5097 ||: 100%|██████████| 60/60 [00:00<00:00, 275.05it/s]\n",
            "                    Training |  Validation\n",
            "accuracy        |     0.650  |     0.750\n",
            "gpu_0_memory_MB |   713.000  |       N/A\n",
            "loss            |     0.622  |     0.510\n",
            "cpu_memory_MB   |  2574.592  |       N/A\n",
            "auc             |     0.490  |     0.500\n",
            "Epoch duration: 0:00:02.120244\n",
            "Estimated training time remaining: 0:03:30\n",
            "Epoch 1/99\n",
            "Peak CPU memory usage MB: 2598.692\n",
            "GPU 0 memory usage MB: 791\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.7104, auc: 0.5516, loss: 0.4589 ||: 100%|██████████| 240/240 [00:01<00:00, 149.90it/s]\n",
            "Validating\n",
            "accuracy: 0.8333, auc: 0.6667, loss: 0.3096 ||: 100%|██████████| 60/60 [00:00<00:00, 338.79it/s]\n",
            "                    Training |  Validation\n",
            "accuracy        |     0.710  |     0.833\n",
            "gpu_0_memory_MB |   791.000  |       N/A\n",
            "loss            |     0.459  |     0.310\n",
            "cpu_memory_MB   |  2598.692  |       N/A\n",
            "auc             |     0.552  |     0.667\n",
            "Epoch duration: 0:00:01.851631\n",
            "Estimated training time remaining: 0:03:14\n",
            "Epoch 2/99\n",
            "Peak CPU memory usage MB: 2599.472\n",
            "GPU 0 memory usage MB: 791\n",
            "Training\n",
            "accuracy: 0.9542, auc: 0.9358, loss: 0.2189 ||: 100%|██████████| 240/240 [00:01<00:00, 143.67it/s]\n",
            "Validating\n",
            "accuracy: 0.9583, auc: 0.9611, loss: 0.1557 ||: 100%|██████████| 60/60 [00:00<00:00, 331.89it/s]\n",
            "                    Training |  Validation\n",
            "accuracy        |     0.954  |     0.958\n",
            "gpu_0_memory_MB |   791.000  |       N/A\n",
            "loss            |     0.219  |     0.156\n",
            "cpu_memory_MB   |  2599.472  |       N/A\n",
            "auc             |     0.936  |     0.961\n",
            "Epoch duration: 0:00:01.927084\n",
            "Estimated training time remaining: 0:03:10\n",
            "Epoch 3/99\n",
            "Peak CPU memory usage MB: 2599.6\n",
            "GPU 0 memory usage MB: 791\n",
            "Training\n",
            "accuracy: 0.9750, auc: 0.9731, loss: 0.1056 ||: 100%|██████████| 240/240 [00:01<00:00, 146.67it/s]\n",
            "Validating\n",
            "accuracy: 0.9667, auc: 0.9667, loss: 0.1080 ||: 100%|██████████| 60/60 [00:00<00:00, 317.07it/s]\n",
            "                    Training |  Validation\n",
            "accuracy        |     0.975  |     0.967\n",
            "gpu_0_memory_MB |   791.000  |       N/A\n",
            "loss            |     0.106  |     0.108\n",
            "cpu_memory_MB   |  2599.600  |       N/A\n",
            "auc             |     0.973  |     0.967\n",
            "Epoch duration: 0:00:01.902997\n",
            "Estimated training time remaining: 0:03:07\n",
            "Epoch 4/99\n",
            "Peak CPU memory usage MB: 2599.636\n",
            "GPU 0 memory usage MB: 791\n",
            "Training\n",
            "accuracy: 0.9812, auc: 0.9794, loss: 0.0661 ||: 100%|██████████| 240/240 [00:01<00:00, 147.18it/s]\n",
            "Validating\n",
            "accuracy: 0.9833, auc: 0.9778, loss: 0.0744 ||: 100%|██████████| 60/60 [00:00<00:00, 334.17it/s]\n",
            "                    Training |  Validation\n",
            "accuracy        |     0.981  |     0.983\n",
            "gpu_0_memory_MB |   791.000  |       N/A\n",
            "loss            |     0.066  |     0.074\n",
            "cpu_memory_MB   |  2599.636  |       N/A\n",
            "auc             |     0.979  |     0.978\n",
            "Epoch duration: 0:00:01.896026\n",
            "Estimated training time remaining: 0:03:04\n",
            "Epoch 5/99\n",
            "Peak CPU memory usage MB: 2599.88\n",
            "GPU 0 memory usage MB: 791\n",
            "Training\n",
            "accuracy: 0.9854, auc: 0.9859, loss: 0.0463 ||: 100%|██████████| 240/240 [00:01<00:00, 145.25it/s]\n",
            "Validating\n",
            "accuracy: 0.9833, auc: 0.9778, loss: 0.0552 ||: 100%|██████████| 60/60 [00:00<00:00, 321.94it/s]\n",
            "                    Training |  Validation\n",
            "accuracy        |     0.985  |     0.983\n",
            "gpu_0_memory_MB |   791.000  |       N/A\n",
            "loss            |     0.046  |     0.055\n",
            "cpu_memory_MB   |  2599.880  |       N/A\n",
            "auc             |     0.986  |     0.978\n",
            "Epoch duration: 0:00:01.915309\n",
            "Estimated training time remaining: 0:03:02\n",
            "Epoch 6/99\n",
            "Peak CPU memory usage MB: 2600.988\n",
            "GPU 0 memory usage MB: 791\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9854, auc: 0.9842, loss: 0.0365 ||: 100%|██████████| 240/240 [00:01<00:00, 142.86it/s]\n",
            "Validating\n",
            "accuracy: 0.9833, auc: 0.9778, loss: 0.0459 ||: 100%|██████████| 60/60 [00:00<00:00, 342.89it/s]\n",
            "                    Training |  Validation\n",
            "accuracy        |     0.985  |     0.983\n",
            "gpu_0_memory_MB |   791.000  |       N/A\n",
            "loss            |     0.036  |     0.046\n",
            "cpu_memory_MB   |  2600.988  |       N/A\n",
            "auc             |     0.984  |     0.978\n",
            "Epoch duration: 0:00:01.947568\n",
            "Estimated training time remaining: 0:03:00\n",
            "Epoch 7/99\n",
            "Peak CPU memory usage MB: 2601.2\n",
            "GPU 0 memory usage MB: 791\n",
            "Training\n",
            "accuracy: 0.9958, auc: 0.9952, loss: 0.0274 ||: 100%|██████████| 240/240 [00:01<00:00, 151.10it/s]\n",
            "Validating\n",
            "accuracy: 0.9917, auc: 0.9833, loss: 0.0341 ||: 100%|██████████| 60/60 [00:00<00:00, 306.16it/s]\n",
            "                    Training |  Validation\n",
            "accuracy        |     0.996  |     0.992\n",
            "gpu_0_memory_MB |   791.000  |       N/A\n",
            "loss            |     0.027  |     0.034\n",
            "cpu_memory_MB   |  2601.200  |       N/A\n",
            "auc             |     0.995  |     0.983\n",
            "Epoch duration: 0:00:01.864971\n",
            "Estimated training time remaining: 0:02:57\n",
            "Epoch 8/99\n",
            "Peak CPU memory usage MB: 2601.216\n",
            "GPU 0 memory usage MB: 791\n",
            "Training\n",
            "accuracy: 0.9958, auc: 0.9952, loss: 0.0187 ||: 100%|██████████| 240/240 [00:01<00:00, 150.55it/s]\n",
            "Validating\n",
            "accuracy: 0.9917, auc: 0.9833, loss: 0.0295 ||: 100%|██████████| 60/60 [00:00<00:00, 321.85it/s]\n",
            "                    Training |  Validation\n",
            "accuracy        |     0.996  |     0.992\n",
            "gpu_0_memory_MB |   791.000  |       N/A\n",
            "loss            |     0.019  |     0.030\n",
            "cpu_memory_MB   |  2601.216  |       N/A\n",
            "auc             |     0.995  |     0.983\n",
            "Epoch duration: 0:00:01.862809\n",
            "Estimated training time remaining: 0:02:55\n",
            "Epoch 9/99\n",
            "Peak CPU memory usage MB: 2601.244\n",
            "GPU 0 memory usage MB: 791\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 0.9958, auc: 0.9952, loss: 0.0148 ||: 100%|██████████| 240/240 [00:01<00:00, 143.86it/s]\n",
            "Validating\n",
            "accuracy: 0.9917, auc: 0.9833, loss: 0.0233 ||: 100%|██████████| 60/60 [00:00<00:00, 324.58it/s]\n",
            "                    Training |  Validation\n",
            "accuracy        |     0.996  |     0.992\n",
            "gpu_0_memory_MB |   791.000  |       N/A\n",
            "loss            |     0.015  |     0.023\n",
            "cpu_memory_MB   |  2601.244  |       N/A\n",
            "auc             |     0.995  |     0.983\n",
            "Epoch duration: 0:00:01.953221\n",
            "Estimated training time remaining: 0:02:53\n",
            "Epoch 10/99\n",
            "Peak CPU memory usage MB: 2601.268\n",
            "GPU 0 memory usage MB: 791\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0112 ||: 100%|██████████| 240/240 [00:01<00:00, 143.55it/s]\n",
            "Validating\n",
            "accuracy: 0.9917, auc: 0.9833, loss: 0.0172 ||: 100%|██████████| 60/60 [00:00<00:00, 334.85it/s]\n",
            "                    Training |  Validation\n",
            "accuracy        |     1.000  |     0.992\n",
            "gpu_0_memory_MB |   791.000  |       N/A\n",
            "loss            |     0.011  |     0.017\n",
            "cpu_memory_MB   |  2601.268  |       N/A\n",
            "auc             |     1.000  |     0.983\n",
            "Epoch duration: 0:00:01.934454\n",
            "Estimated training time remaining: 0:02:51\n",
            "Epoch 11/99\n",
            "Peak CPU memory usage MB: 2601.304\n",
            "GPU 0 memory usage MB: 791\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0087 ||: 100%|██████████| 240/240 [00:01<00:00, 145.98it/s]\n",
            "Validating\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0137 ||: 100%|██████████| 60/60 [00:00<00:00, 337.51it/s]\n",
            "                    Training |  Validation\n",
            "accuracy        |     1.000  |     1.000\n",
            "gpu_0_memory_MB |   791.000  |       N/A\n",
            "loss            |     0.009  |     0.014\n",
            "cpu_memory_MB   |  2601.304  |       N/A\n",
            "auc             |     1.000  |     1.000\n",
            "Epoch duration: 0:00:01.905587\n",
            "Estimated training time remaining: 0:02:49\n",
            "Epoch 12/99\n",
            "Peak CPU memory usage MB: 2601.328\n",
            "GPU 0 memory usage MB: 791\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0066 ||: 100%|██████████| 240/240 [00:01<00:00, 147.48it/s]\n",
            "Validating\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0115 ||: 100%|██████████| 60/60 [00:00<00:00, 333.76it/s]\n",
            "                    Training |  Validation\n",
            "accuracy        |     1.000  |     1.000\n",
            "gpu_0_memory_MB |   791.000  |       N/A\n",
            "loss            |     0.007  |     0.012\n",
            "cpu_memory_MB   |  2601.328  |       N/A\n",
            "auc             |     1.000  |     1.000\n",
            "Epoch duration: 0:00:01.888614\n",
            "Estimated training time remaining: 0:02:47\n",
            "Epoch 13/99\n",
            "Peak CPU memory usage MB: 2601.328\n",
            "GPU 0 memory usage MB: 791\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0057 ||: 100%|██████████| 240/240 [00:01<00:00, 150.62it/s]\n",
            "Validating\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0095 ||: 100%|██████████| 60/60 [00:00<00:00, 343.63it/s]\n",
            "                    Training |  Validation\n",
            "accuracy        |     1.000  |     1.000\n",
            "gpu_0_memory_MB |   791.000  |       N/A\n",
            "loss            |     0.006  |     0.009\n",
            "cpu_memory_MB   |  2601.328  |       N/A\n",
            "auc             |     1.000  |     1.000\n",
            "Epoch duration: 0:00:01.841577\n",
            "Estimated training time remaining: 0:02:44\n",
            "Epoch 14/99\n",
            "Peak CPU memory usage MB: 2601.328\n",
            "GPU 0 memory usage MB: 791\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0040 ||: 100%|██████████| 240/240 [00:01<00:00, 151.65it/s]\n",
            "Validating\n",
            "  0%|          | 0/60 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0075 ||: 100%|██████████| 60/60 [00:00<00:00, 310.47it/s]\n",
            "                    Training |  Validation\n",
            "accuracy        |     1.000  |     1.000\n",
            "gpu_0_memory_MB |   791.000  |       N/A\n",
            "loss            |     0.004  |     0.007\n",
            "cpu_memory_MB   |  2601.328  |       N/A\n",
            "auc             |     1.000  |     1.000\n",
            "Epoch duration: 0:00:01.855895\n",
            "Estimated training time remaining: 0:02:42\n",
            "Epoch 15/99\n",
            "Peak CPU memory usage MB: 2601.34\n",
            "GPU 0 memory usage MB: 791\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0036 ||: 100%|██████████| 240/240 [00:01<00:00, 151.79it/s]\n",
            "Validating\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0062 ||: 100%|██████████| 60/60 [00:00<00:00, 324.89it/s]\n",
            "                    Training |  Validation\n",
            "accuracy        |     1.000  |     1.000\n",
            "gpu_0_memory_MB |   791.000  |       N/A\n",
            "loss            |     0.004  |     0.006\n",
            "cpu_memory_MB   |  2601.340  |       N/A\n",
            "auc             |     1.000  |     1.000\n",
            "Epoch duration: 0:00:01.852063\n",
            "Estimated training time remaining: 0:02:40\n",
            "Epoch 16/99\n",
            "Peak CPU memory usage MB: 2601.34\n",
            "GPU 0 memory usage MB: 791\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0030 ||: 100%|██████████| 240/240 [00:01<00:00, 149.46it/s]\n",
            "Validating\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0049 ||: 100%|██████████| 60/60 [00:00<00:00, 334.43it/s]\n",
            "                    Training |  Validation\n",
            "accuracy        |     1.000  |     1.000\n",
            "gpu_0_memory_MB |   791.000  |       N/A\n",
            "loss            |     0.003  |     0.005\n",
            "cpu_memory_MB   |  2601.340  |       N/A\n",
            "auc             |     1.000  |     1.000\n",
            "Epoch duration: 0:00:01.873357\n",
            "Estimated training time remaining: 0:02:38\n",
            "Epoch 17/99\n",
            "Peak CPU memory usage MB: 2601.34\n",
            "GPU 0 memory usage MB: 791\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0024 ||: 100%|██████████| 240/240 [00:01<00:00, 148.17it/s]\n",
            "Validating\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0040 ||: 100%|██████████| 60/60 [00:00<00:00, 338.98it/s]\n",
            "                    Training |  Validation\n",
            "accuracy        |     1.000  |     1.000\n",
            "gpu_0_memory_MB |   791.000  |       N/A\n",
            "loss            |     0.002  |     0.004\n",
            "cpu_memory_MB   |  2601.340  |       N/A\n",
            "auc             |     1.000  |     1.000\n",
            "Epoch duration: 0:00:01.874687\n",
            "Estimated training time remaining: 0:02:36\n",
            "Epoch 18/99\n",
            "Peak CPU memory usage MB: 2601.352\n",
            "GPU 0 memory usage MB: 791\n",
            "Training\n",
            "  0%|          | 0/240 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0020 ||: 100%|██████████| 240/240 [00:01<00:00, 142.46it/s]\n",
            "Validating\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0035 ||: 100%|██████████| 60/60 [00:00<00:00, 321.57it/s]\n",
            "                    Training |  Validation\n",
            "accuracy        |     1.000  |     1.000\n",
            "gpu_0_memory_MB |   791.000  |       N/A\n",
            "loss            |     0.002  |     0.004\n",
            "cpu_memory_MB   |  2601.352  |       N/A\n",
            "auc             |     1.000  |     1.000\n",
            "Epoch duration: 0:00:01.954735\n",
            "Estimated training time remaining: 0:02:34\n",
            "Epoch 19/99\n",
            "Peak CPU memory usage MB: 2601.368\n",
            "GPU 0 memory usage MB: 791\n",
            "Training\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0018 ||: 100%|██████████| 240/240 [00:01<00:00, 147.03it/s]\n",
            "Validating\n",
            "  0%|          | 0/60 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "accuracy: 1.0000, auc: 1.0000, loss: 0.0030 ||: 100%|██████████| 60/60 [00:00<00:00, 330.38it/s]\n",
            "Ran out of patience.  Stopping training.\n",
            "cannot load best weights without `serialization_dir`, so you're just getting the last weights\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best_epoch': 11,\n",
              " 'best_validation_accuracy': 1.0,\n",
              " 'best_validation_auc': 1.0,\n",
              " 'best_validation_loss': 0.013716259847084682,\n",
              " 'epoch': 18,\n",
              " 'peak_cpu_memory_MB': 2601.368,\n",
              " 'peak_gpu_0_memory_MB': 791,\n",
              " 'training_accuracy': 1.0,\n",
              " 'training_auc': 1.0,\n",
              " 'training_cpu_memory_MB': 2601.352,\n",
              " 'training_duration': '0:00:36.264026',\n",
              " 'training_epochs': 18,\n",
              " 'training_gpu_0_memory_MB': 791,\n",
              " 'training_loss': 0.001990130916237831,\n",
              " 'training_start_epoch': 0,\n",
              " 'validation_accuracy': 1.0,\n",
              " 'validation_auc': 1.0,\n",
              " 'validation_loss': 0.003542238970597585}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX1_Uct67zfU",
        "colab_type": "text"
      },
      "source": [
        "## Prediction (For Lv1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmmvtKLVKzQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "\n",
        "predictor = Predictor(model, reader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ncxwf2ONOl9",
        "colab_type": "code",
        "outputId": "2e7943c1-0917-4ade-f8d6-517e4611dd55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "level1_pred = []\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "TEST_FILE = 'level1_test.tsv'\n",
        "TEST_PATH = os.path.join(DATA_DIR, TEST_FILE)\n",
        "test_dataset = reader.read(TEST_PATH)\n",
        "\n",
        "for instance in tqdm(test_dataset):\n",
        "    ans = predictor.predict_instance(instance)\n",
        "    level1_pred.append(ans['label'])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200it [00:00, 6657.52it/s]\n",
            "100%|██████████| 200/200 [00:00<00:00, 371.92it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mKQ-ARc3S9L",
        "colab_type": "code",
        "outputId": "59ffa344-95a3-4c6e-e507-415e2f551faf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "level1_pred_df = pd.DataFrame(level1_pred)\n",
        "level1_pred_df.T"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 200 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  0   1   2   3   4   5   6   7   8    ... 191 192 193 194 195 196 197 198 199\n",
              "0   1   1   1   1   1   0   1   0   1  ...   1   1   1   1   1   1   0   1   0\n",
              "\n",
              "[1 rows x 200 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W-V1q24lBgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "level1_pred_df.T.to_csv('level1_pred.csv', index = False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VWBm8RP3Sze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('level1_pred.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}